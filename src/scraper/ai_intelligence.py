"""
游뱄 AI-DRIVEN INTELLIGENT SCRAPING MODULE
Revolution칛r AI-integration f칬r automatisk web scraping
"""

import asyncio
import json
import re
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Tuple, Union
from abc import ABC, abstractmethod
import base64
import io
from pathlib import Path

import aiohttp
import openai
from PIL import Image
import cv2
import numpy as np
from bs4 import BeautifulSoup
import pytesseract
from transformers import pipeline
import torch

from ..utils.config import Config
from ..utils.logging import ScrapingLogger


class MockOpenAIClient:
    """Mock OpenAI-klient f칬r demo-syfte"""

    class MockMessage:
        def __init__(self, content: str):
            self.content = content

    class MockChoice:
        def __init__(self, content: str):
            self.message = MockOpenAIClient.MockMessage(content)

    class MockResponse:
        def __init__(self, content: str):
            self.choices = [MockOpenAIClient.MockChoice(content)]

    class MockCompletions:
        async def create(self, **kwargs):
            # Simulera AI-svar baserat p친 prompt
            prompt = kwargs.get("messages", [{}])[-1].get("content", "")

            if "selectors" in prompt.lower():
                mock_response = """
                {
                    "primary_selectors": {
                        "title": ".product-title, h1",
                        "price": ".price, .cost, [class*='price']",
                        "description": ".description, .product-desc"
                    },
                    "fallback_selectors": {
                        "title": ["h1", "h2", ".title"],
                        "price": [".price", ".cost", "[data-price]"]
                    },
                    "reasoning": "AI-genererade selectors baserat p친 vanliga HTML-m칬nster",
                    "confidence": 0.85
                }
                """
            elif "analysera" in prompt.lower():
                mock_response = """
                {
                    "intent": "hitta produktpris",
                    "entity": "iPhone 15",
                    "data_type": "pris",
                    "website_hint": "amazon",
                    "expected_format": "numeriskt v칛rde med valuta"
                }
                """
            else:
                mock_response = """
                Baserat p친 den tillhandah친llna datan kan jag ge f칬ljande insikter:
                
                1. Prism칬nster: iPhone 15 칛r prisat h칬gre 칛n Samsung Galaxy med $100 skillnad
                2. Betyg: B친da produkterna har h칬ga betyg 칬ver 4.0, vilket indikerar h칬g kundn칬jdhet
                3. Marknadsposition: Apple forts칛tter att ha premium-priss칛ttning
                4. Rekommendation: 칐vervaka konkurrentpriser f칬r marknadsanalys
                5. Trend: Smartphones 칬ver $800 visar stark efterfr친gan
                """

            return MockOpenAIClient.MockResponse(mock_response)

    class MockChat:
        def __init__(self):
            self.completions = MockOpenAIClient.MockCompletions()

    def __init__(self):
        self.chat = self.MockChat()


@dataclass
class AIScrapingRequest:
    """AI-scraping request med naturligt spr친k"""

    query: str
    target_url: Optional[str] = None
    context: Optional[str] = None
    expected_data_type: Optional[str] = None
    confidence_threshold: float = 0.8


@dataclass
class AIScrapingResult:
    """Resultat fr친n AI-driven scraping"""

    success: bool
    data: Dict[str, Any]
    selectors_used: Dict[str, str]
    confidence_score: float
    reasoning: str
    alternative_selectors: List[Dict[str, str]] = field(default_factory=list)
    ai_suggestions: List[str] = field(default_factory=list)


class BaseAIIntelligence(ABC):
    """Abstrakt basklass f칬r AI-intelligens"""

    def __init__(self, config: Optional[Config] = None):
        self.config = config or Config()
        self.logger = ScrapingLogger(__name__)
        self.openai_client = None
        self._setup_ai_models()

    def _setup_ai_models(self):
        """Konfigurera AI-modeller"""
        try:
            api_key = self.config.get("ai.openai_api_key")
            if api_key:
                openai.api_key = api_key
                self.openai_client = openai.AsyncOpenAI(api_key=api_key)
            else:
                self.logger.info("Anv칛nder mock OpenAI-klient f칬r demo")
                self.openai_client = MockOpenAIClient()
        except Exception as e:
            self.logger.warning(f"Kunde inte konfigurera OpenAI, anv칛nder mock: {e}")
            self.openai_client = MockOpenAIClient()

    @abstractmethod
    async def process(self, content: str, request: AIScrapingRequest) -> AIScrapingResult:
        """Processa inneh친ll med AI"""
        # Implementera bas-processering
        selectors = await self.analyze_page_structure(content, request.query)
        return AIScrapingResult(
            success=True,
            data={"selectors": selectors},
            selectors_used=selectors,
            confidence_score=0.8,
            reasoning="AI-genererade selectors fr친n HTML-inneh친ll",
        )


class GPT4SelectorGenerator(BaseAIIntelligence):
    """GPT-4 driven automatisk selector-generering"""

    def __init__(self, config: Optional[Config] = None):
        super().__init__(config)
        self.selector_cache = {}
        self.learning_history = []

    async def analyze_page_structure(self, html_content: str, target_data: str) -> Dict[str, str]:
        """
        Analysera HTML-struktur och generera optimala selectors
        """
        # OpenAI-klient finns alltid (antingen verklig eller mock)

        # Skapa prompt f칬r GPT-4
        prompt = self._create_analysis_prompt(html_content, target_data)

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {
                        "role": "system",
                        "content": "Du 칛r en expert p친 web scraping och HTML-analys. Generera CSS/XPath selectors f칬r att extrahera specifik data.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=0.1,
                max_tokens=1000,
            )

            result = response.choices[0].message.content
            selectors = self._parse_selector_response(result)

            # Cache resultatet
            cache_key = f"{hash(html_content)}:{target_data}"
            self.selector_cache[cache_key] = selectors

            return selectors

        except Exception as e:
            self.logger.error(f"GPT-4 selector-generering misslyckades: {e}")
            return {}

    def _create_analysis_prompt(self, html_content: str, target_data: str) -> str:
        """Skapa intelligent prompt f칬r GPT-4"""
        return f"""
        Analysera f칬ljande HTML och generera CSS/XPath selectors f칬r att extrahera: "{target_data}"
        
        HTML:
        {html_content[:5000]}  # Begr칛nsa f칬r token-limit
        
        Instruktioner:
        1. Identifiera element som inneh친ller m친l-data
        2. Generera robusta selectors som fungerar 칛ven vid mindre 칛ndringar
        3. Prioritera CSS selectors 칬ver XPath n칛r m칬jligt
        4. Inkludera fallback-selectors
        5. F칬rklara varf칬r varje selector valdes
        
        Returnera resultatet i JSON-format:
        {{
            "primary_selectors": {{
                "title": "selector",
                "price": "selector",
                "description": "selector"
            }},
            "fallback_selectors": {{
                "title": ["selector1", "selector2"],
                "price": ["selector1", "selector2"]
            }},
            "reasoning": "F칬rklaring av valda selectors",
            "confidence": 0.95
        }}
        """

    def _parse_selector_response(self, response: str) -> Dict[str, str]:
        """Parsa GPT-4 svar till selectors"""
        try:
            # Extrahera JSON fr친n response
            json_match = re.search(r"\{.*\}", response, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
                return data.get("primary_selectors", {})
        except Exception as e:
            self.logger.error(f"Kunde inte parsa selector-response: {e}")

        return {}

    async def self_improving_selectors(
        self, url: str, selectors: Dict[str, str], success_rate: float
    ) -> Dict[str, str]:
        """
        F칬rb칛ttra selectors baserat p친 framg친ngsgrad
        """
        if success_rate < 0.8:  # Om framg친ngsgrad 칛r l친g
            self.learning_history.append(
                {
                    "url": url,
                    "selectors": selectors,
                    "success_rate": success_rate,
                    "timestamp": asyncio.get_event_loop().time(),
                }
            )

            # Analysera m칬nster i misslyckade f칬rs칬k
            improved_selectors = await self._learn_from_failures(url, selectors)
            return improved_selectors

        return selectors

    async def _learn_from_failures(self, url: str, selectors: Dict[str, str]) -> Dict[str, str]:
        """L칛r fr친n misslyckade scraping-f칬rs칬k"""
        # H칛r skulle vi implementera ML-algoritmer f칬r att f칬rb칛ttra selectors
        # F칬r nu returnerar vi f칬rb칛ttrade versioner baserat p친 heuristik
        improved = {}
        for key, selector in selectors.items():
            # L칛gg till fallback-selectors
            if "class" in selector:
                improved[key] = f"{selector}, [class*='{selector.split('.')[-1]}']"
            else:
                improved[key] = selector

        return improved

    async def process(self, content: str, request: AIScrapingRequest) -> AIScrapingResult:
        """Processa inneh친ll med GPT-4 selector-generering"""
        selectors = await self.analyze_page_structure(content, request.query)
        return AIScrapingResult(
            success=True,
            data={"selectors": selectors},
            selectors_used=selectors,
            confidence_score=0.85,
            reasoning="GPT-4 genererade selectors fr친n HTML-inneh친ll",
        )


class ComputerVisionScraper(BaseAIIntelligence):
    """Computer vision f칬r dynamisk content detection"""

    def __init__(self, config: Optional[Config] = None):
        super().__init__(config)
        self.ocr_pipeline = None
        self.object_detection_model = None
        self._setup_cv_models()

    def _setup_cv_models(self):
        """Konfigurera computer vision modeller"""
        try:
            # OCR pipeline
            self.ocr_pipeline = pipeline("text-generation", model="gpt2")

            # Object detection (f칬renklad version)
            try:
                cascade_path = cv2.data.haarcascades + "haarcascade_frontalface_default.xml"
                self.object_detection_model = cv2.CascadeClassifier(cascade_path)
                if self.object_detection_model.empty():
                    self.logger.warning("Kunde inte ladda cascade-fil, anv칛nder alternativ metod")
                    self.object_detection_model = None
            except Exception as e:
                self.logger.warning(f"Cascade-fil misslyckades, anv칛nder alternativ: {e}")
                self.object_detection_model = None
        except Exception as e:
            self.logger.warning(f"Kunde inte konfigurera CV-modeller: {e}")

    async def extract_text_from_images(self, image_data: bytes) -> List[str]:
        """OCR f칬r att l칛sa text fr친n bilder"""
        try:
            # Konvertera bytes till PIL Image
            image = Image.open(io.BytesIO(image_data))

            # Konvertera till OpenCV format
            cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

            # OCR med Tesseract
            text = pytesseract.image_to_string(cv_image, lang="eng+swe")

            # Rensa och strukturera text
            lines = [line.strip() for line in text.split("\n") if line.strip()]

            return lines

        except Exception as e:
            self.logger.error(f"OCR misslyckades: {e}")
            return []

    async def detect_ui_elements(self, screenshot: bytes) -> Dict[str, List[Tuple[int, int, int, int]]]:
        """Identifierar UI-element via computer vision"""
        try:
            image = Image.open(io.BytesIO(screenshot))
            cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

            # Konvertera till gr친skala
            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)

            # Detektera konturer (f칬r att hitta knappar, formul칛r, etc.)
            _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)
            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            ui_elements = {"buttons": [], "forms": [], "tables": [], "text_areas": []}

            for contour in contours:
                x, y, w, h = cv2.boundingRect(contour)
                area = w * h

                # Klassificera element baserat p친 storlek och form
                if 50 < area < 5000:  # Knapp-storlek
                    ui_elements["buttons"].append((x, y, w, h))
                elif area > 10000:  # St칬rre element
                    aspect_ratio = w / h
                    if 0.5 < aspect_ratio < 2.0:
                        ui_elements["forms"].append((x, y, w, h))
                    else:
                        ui_elements["tables"].append((x, y, w, h))

            return ui_elements

        except Exception as e:
            self.logger.error(f"UI-element detection misslyckades: {e}")
            return {}

    async def process(self, content: str, request: AIScrapingRequest) -> AIScrapingResult:
        """Processa inneh친ll med computer vision"""
        # Implementation f칬r computer vision processing
        pass


class NaturalLanguageQueryInterface(BaseAIIntelligence):
    """Naturligt spr친k query interface"""

    def __init__(self, config: Optional[Config] = None):
        super().__init__(config)
        self.selector_generator = GPT4SelectorGenerator(config)
        self.query_cache = {}

    async def query(self, question: str, target_url: Optional[str] = None) -> AIScrapingResult:
        """
        St칛ll fr친gor p친 naturligt spr친k
        Exempel: "Vad kostar iPhone 15 p친 Amazon?"
        """
        # Analysera fr친gan
        parsed_query = await self._parse_natural_language_query(question)

        # Hitta r칛tt webbplats om inte specificerad
        if not target_url:
            target_url = await self._find_relevant_website(parsed_query)

        # Generera selectors baserat p친 fr친gan
        selectors = await self._generate_selectors_from_query(parsed_query, target_url)

        # Utf칬r scraping
        result = await self._execute_scraping(target_url, selectors, parsed_query)

        return result

    async def _parse_natural_language_query(self, question: str) -> Dict[str, Any]:
        """Parsa naturligt spr친k till strukturerad data"""
        # OpenAI-klient finns alltid (antingen verklig eller mock)

        prompt = f"""
        Analysera f칬ljande fr친ga och extrahera strukturerad information:
        
        Fr친ga: "{question}"
        
        Returnera JSON med:
        {{
            "intent": "vad anv칛ndaren vill veta",
            "entity": "vad som s칬ks efter",
            "data_type": "pris|recension|beskrivning|etc",
            "website_hint": "f칬reslagna webbplatser",
            "expected_format": "svar-format"
        }}
        """

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "Du 칛r en expert p친 att analysera naturligt spr친k f칬r web scraping."},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.1,
            )

            result = response.choices[0].message.content
            return json.loads(result)

        except Exception as e:
            self.logger.error(f"Query parsing misslyckades: {e}")
            return {"intent": "unknown", "entity": question}

    async def _find_relevant_website(self, parsed_query: Dict[str, Any]) -> str:
        """Hitta relevant webbplats baserat p친 query"""
        # Enkel heuristik f칬r att hitta webbplatser
        entity = parsed_query.get("entity", "").lower()
        website_hint = parsed_query.get("website_hint", "")

        # Mappning av vanliga webbplatser
        website_mapping = {
            "amazon": "https://www.amazon.com",
            "ebay": "https://www.ebay.com",
            "google": "https://www.google.com",
            "wikipedia": "https://www.wikipedia.org",
        }

        for keyword, url in website_mapping.items():
            if keyword in entity or keyword in website_hint:
                return url

        # Fallback till Google
        return "https://www.google.com"

    async def _generate_selectors_from_query(self, parsed_query: Dict[str, Any], url: str) -> Dict[str, str]:
        """Generera selectors baserat p친 parsed query"""
        intent = parsed_query.get("intent", "")
        data_type = parsed_query.get("data_type", "")

        # Mappning av vanliga data-typer till selectors
        selector_mapping = {
            "pris": {
                "price": ".price, .cost, [class*='price'], [class*='cost']",
                "currency": "[class*='currency'], .currency",
            },
            "recension": {
                "rating": ".rating, .stars, [class*='rating']",
                "review_text": ".review, .comment, [class*='review']",
            },
            "beskrivning": {
                "description": ".description, .details, [class*='description']",
                "title": ".title, .name, h1, h2",
            },
        }

        return selector_mapping.get(data_type, {})

    async def _execute_scraping(
        self, url: str, selectors: Dict[str, str], parsed_query: Dict[str, Any]
    ) -> AIScrapingResult:
        """Utf칬r faktisk scraping"""
        # H칛r skulle vi integrera med huvud-scraping-modulen
        # F칬r nu returnerar vi en mock-resultat

        return AIScrapingResult(
            success=True,
            data={"answer": f"Simulerat svar f칬r: {parsed_query.get('intent', '')}"},
            selectors_used=selectors,
            confidence_score=0.9,
            reasoning="AI-genererat svar baserat p친 naturligt spr친k query",
        )

    async def process(self, content: str, request: AIScrapingRequest) -> AIScrapingResult:
        """Processa inneh친ll med naturligt spr친k"""
        return await self.query(request.query, request.target_url)


class AIIntelligenceOrchestrator:
    """Orchestrator f칬r alla AI-intelligens moduler"""

    def __init__(self, config: Optional[Config] = None):
        self.config = config or Config()
        self.logger = ScrapingLogger(__name__)

        # Initiera alla AI-moduler
        self.gpt4_generator = GPT4SelectorGenerator(config)
        self.cv_scraper = ComputerVisionScraper(config)
        self.nl_interface = NaturalLanguageQueryInterface(config)

        # AI-modeller pipeline
        self.ai_pipeline = [self.gpt4_generator, self.cv_scraper, self.nl_interface]

    async def intelligent_scrape(self, request: AIScrapingRequest, html_content: str = None) -> AIScrapingResult:
        """
        Intelligent scraping med alla AI-moduler
        """
        self.logger.info(f"Startar intelligent scraping f칬r: {request.query}")

        # F칬rs칬k med naturligt spr친k f칬rst
        if "?" in request.query or any(
            word in request.query.lower() for word in ["vad", "hur", "n칛r", "var", "vilken"]
        ):
            return await self.nl_interface.query(request.query, request.target_url)

        # Annars anv칛nd GPT-4 f칬r selector-generering
        if html_content:
            selectors = await self.gpt4_generator.analyze_page_structure(html_content, request.query)

            # Utf칬r scraping med genererade selectors
            result = await self._execute_with_selectors(request.target_url, selectors)

            # F칬rb칛ttra selectors baserat p친 resultat
            improved_selectors = await self.gpt4_generator.self_improving_selectors(
                request.target_url, selectors, result.confidence_score
            )

            if improved_selectors != selectors:
                result = await self._execute_with_selectors(request.target_url, improved_selectors)
                result.selectors_used = improved_selectors

            return result

        return AIScrapingResult(
            success=False,
            data={},
            selectors_used={},
            confidence_score=0.0,
            reasoning="Inget HTML-inneh친ll tillhandah친llet",
        )

    async def _execute_with_selectors(self, url: str, selectors: Dict[str, str]) -> AIScrapingResult:
        """Utf칬r scraping med givna selectors"""
        # H칛r skulle vi integrera med huvud-scraping-modulen
        # F칬r nu returnerar vi en mock-resultat

        return AIScrapingResult(
            success=True,
            data={"extracted_data": "Simulerat data fr친n AI-selectors"},
            selectors_used=selectors,
            confidence_score=0.85,
            reasoning="AI-genererade selectors anv칛ndes framg친ngsrikt",
        )

    async def get_ai_insights(self, scraped_data: Dict[str, Any]) -> List[str]:
        """F친 AI-insikter fr친n scraped data"""
        # OpenAI-klient finns alltid (antingen verklig eller mock)

        try:
            prompt = f"""
            Analysera f칬ljande scraped data och ge insikter:
            
            Data: {json.dumps(scraped_data, indent=2)}
            
            Ge 3-5 insikter om:
            1. M칬nster i datan
            2. Avvikelser eller intressanta observationer
            3. Rekommendationer f칬r vidare analys
            """

            response = await self.gpt4_generator.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "Du 칛r en expert p친 data-analys och insikter."},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.3,
            )

            insights = response.choices[0].message.content.split("\n")
            return [insight.strip() for insight in insights if insight.strip()]

        except Exception as e:
            self.logger.error(f"AI-insikter misslyckades: {e}")
            return []


# Exempel p친 anv칛ndning
async def demo_ai_intelligence():
    """Demo av AI-intelligens funktioner"""

    # Konfigurera med OpenAI API key
    config = Config()
    config.set("ai.openai_api_key", "your-openai-api-key-here")

    orchestrator = AIIntelligenceOrchestrator(config)

    # Exempel 1: Naturligt spr친k query
    request1 = AIScrapingRequest(query="Vad kostar iPhone 15 p친 Amazon?", target_url="https://www.amazon.com")

    result1 = await orchestrator.intelligent_scrape(request1)
    print(f"Resultat 1: {result1.data}")

    # Exempel 2: AI-selector generering
    html_content = """
    <html>
        <body>
            <div class="product">
                <h1 class="product-title">iPhone 15</h1>
                <span class="price">$999</span>
                <p class="description">Latest iPhone model</p>
            </div>
        </body>
    </html>
    """

    request2 = AIScrapingRequest(query="Extrahera produktinformation", target_url="https://example.com")

    result2 = await orchestrator.intelligent_scrape(request2, html_content)
    print(f"Resultat 2: {result2.selectors_used}")

    # Exempel 3: AI-insikter
    scraped_data = {
        "products": [
            {"name": "iPhone 15", "price": 999, "rating": 4.5},
            {"name": "Samsung Galaxy", "price": 899, "rating": 4.3},
        ]
    }

    insights = await orchestrator.get_ai_insights(scraped_data)
    print(f"AI-insikter: {insights}")


if __name__ == "__main__":
    asyncio.run(demo_ai_intelligence())
