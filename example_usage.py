#!/usr/bin/env python3
"""
Exempel p√• anv√§ndning av Web Scraping Toolkit

Detta exempel visar hur man anv√§nder verktyget f√∂r att skrapa data fr√•n webbplatser
p√• ett etiskt och effektivt s√§tt.
"""

import asyncio
import sys
from pathlib import Path

# L√§gg till src i Python-s√∂kv√§gen
sys.path.insert(0, str(Path(__file__).parent / "src"))

from scraper import WebScraper
from utils.logging import get_scraping_logger


async def basic_scraping_example():
    """Grundl√§ggande exempel p√• web scraping"""
    print("üöÄ Startar grundl√§ggande scraping-exempel...")

    # Skapa scraper-instans med standardkonfiguration
    async with WebScraper("config/default.yaml") as scraper:
        # Skrapa en enskild URL
        url = "https://httpbin.org/html"
        result = await scraper.scrape_url(url)

        if result.success:
            print(f"‚úÖ Framg√•ngsrikt skrapat: {url}")
            print(f"   Titel: {result.data.get('title', 'N/A')}")
            print(f"   Response-tid: {result.response_time:.2f}s")
        else:
            print(f"‚ùå Fel vid scraping: {result.error}")

        # Exportera data
        scraper.export_data(result, format="json", output_path="data/basic_example.json")


async def batch_scraping_example():
    """Exempel p√• batch-scraping av flera URLs"""
    print("\nüì¶ Startar batch-scraping exempel...")

    # Lista med URLs att skrapa
    urls = [
        "https://httpbin.org/html",
        "https://httpbin.org/json",
        "https://httpbin.org/xml",
    ]

    async with WebScraper("config/default.yaml") as scraper:
        # Skrapa flera URLs parallellt
        results = await scraper.scrape_multiple(urls)

        # Analysera resultat
        successful = [r for r in results if r.success]
        failed = [r for r in results if not r.success]

        print(f"‚úÖ Framg√•ngsrika requests: {len(successful)}")
        print(f"‚ùå Misslyckade requests: {len(failed)}")

        # Exportera alla resultat
        scraper.export_data(results, format="json", output_path="data/batch_example.json")


async def custom_parser_example():
    """Exempel med anpassad HTML-parsing"""
    print("\nüîß Startar anpassad parser-exempel...")

    # Definiera CSS-selektorer f√∂r specifika element
    selectors = {
        "title": "title",
        "headings": "h1, h2, h3",
        "links": "a[href]",
        "images": "img[src]",
    }

    async with WebScraper("config/default.yaml") as scraper:
        url = "https://httpbin.org/html"
        result = await scraper.scrape_url(url)

        if result.success:
            # Anv√§nd anpassade selektorer
            from scraper.parsers import HTMLParser

            parser = HTMLParser()

            # H√§mta r√• HTML och parsa med selektorer
            # (I en riktig implementation skulle detta integreras i WebScraper)
            print(f"‚úÖ Anpassad parsing av: {url}")
            print(f"   Data extraherad: {len(result.data)} f√§lt")


async def data_validation_example():
    """Exempel p√• data-validering"""
    print("\n‚úÖ Startar data-validering exempel...")

    from scraper.validators import DataValidator

    # Exempel-data att validera
    sample_data = {
        "title": "Exempel Titel",
        "email": "test@example.com",
        "phone": "123-456-7890",
        "url": "https://example.com",
        "price": "99.99",
        "description": "En beskrivning av produkten.",
    }

    # Validera data
    validator = DataValidator()
    validated_data = validator.validate(sample_data)

    # Visa valideringsresultat
    validation = validated_data.get("_validation", {})
    print(f"‚úÖ Validering slutf√∂rd:")
    print(f"   Totalt f√§lt: {validation.get('total_fields', 0)}")
    print(f"   Giltiga f√§lt: {validation.get('valid_fields', 0)}")
    print(f"   √ñvergripande giltig: {validation.get('overall_valid', False)}")


async def export_formats_example():
    """Exempel p√• olika export-format"""
    print("\nüì§ Startar export-format exempel...")

    from scraper.exporters import DataExporter

    # Exempel-data
    sample_data = [
        {"id": 1, "name": "Produkt A", "price": 99.99, "category": "Elektronik"},
        {"id": 2, "name": "Produkt B", "price": 149.99, "category": "Kl√§der"},
    ]

    exporter = DataExporter()

    # Exportera till olika format
    formats = ["json", "csv", "xml"]

    for format in formats:
        output_path = f"data/export_example.{format}"
        success = exporter.export(sample_data, format, output_path)

        if success:
            print(f"‚úÖ Exporterat till {format.upper()}: {output_path}")
        else:
            print(f"‚ùå Fel vid export till {format.upper()}")


async def monitoring_example():
    """Exempel p√• √∂vervakning och loggning"""
    print("\nüìä Startar √∂vervakning-exempel...")

    # Skapa specialiserad logger
    logger = get_scraping_logger("monitoring_example")

    # Simulera en scraping-session
    session_id = "example_session_001"
    urls = ["https://httpbin.org/html", "https://httpbin.org/json"]

    logger.start_session(session_id, urls)

    async with WebScraper("config/default.yaml") as scraper:
        for url in urls:
            logger.log_request(url)
            result = await scraper.scrape_url(url)

            if result.success:
                logger.log_response(url, 200, result.response_time, True)
                logger.log_data_extracted(url, len(result.data), "html")
            else:
                logger.log_response(url, 0, result.response_time, False)
                logger.log_error(url, result.error, "scraping_error")

    logger.end_session(session_id)


async def main():
    """Huvudfunktion som k√∂r alla exempel"""
    print("üéØ Web Scraping Toolkit - Anv√§ndningsexempel")
    print("=" * 50)

    try:
        # K√∂r alla exempel
        await basic_scraping_example()
        await batch_scraping_example()
        await custom_parser_example()
        await data_validation_example()
        await export_formats_example()
        await monitoring_example()

        print("\nüéâ Alla exempel slutf√∂rda!")
        print("\nüìÅ Skapade filer:")
        print("   - data/basic_example.json")
        print("   - data/batch_example.json")
        print("   - data/export_example.json")
        print("   - data/export_example.csv")
        print("   - data/export_example.xml")
        print("   - logs/scraper.log")

    except Exception as e:
        print(f"\n‚ùå Fel i exempel: {str(e)}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    # K√∂r huvudfunktionen
    asyncio.run(main())
