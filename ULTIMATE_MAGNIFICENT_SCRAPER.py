#!/usr/bin/env python3
"""
üåü ULTIMATE MAGNIFICENT SCRAPER - Det verkliga ultimata scraping-verktyget!
Magnifikt, utomordentligt, med alla de b√§sta funktionerna som lovades!
"""

import time
import json
import hashlib
import sqlite3
import threading
import queue
import pickle
import gzip
import re
from datetime import datetime
from typing import Dict, Any, List
from enum import Enum
from dataclasses import dataclass
import requests
from bs4 import BeautifulSoup
import uvicorn
from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from urllib.parse import urljoin, urlparse
import tempfile
import os

# ============================================================================
# üéØ ULTIMATE SCRAPER - Avancerade komponenter
# ============================================================================


class ScrapingMode(Enum):
    INTELLIGENT = "intelligent"
    AGGRESSIVE = "aggressive"
    STEALTH = "stealth"
    AI_POWERED = "ai_powered"
    MAGNIFICENT = "magnificent"


class DataType(Enum):
    TEXT = "text"
    IMAGES = "images"
    LINKS = "links"
    FORMS = "forms"
    TABLES = "tables"
    JSON = "json"
    XML = "xml"
    PDF = "pdf"
    VIDEO = "video"
    AUDIO = "audio"


@dataclass
class ScrapingResult:
    url: str
    success: bool
    data: Dict[str, Any]
    metadata: Dict[str, Any]
    performance: Dict[str, float]
    ai_insights: Dict[str, Any]
    security_analysis: Dict[str, Any]
    timestamp: datetime
    hash: str
    quality_score: float


class MagnificentAI:
    """Magnifik AI-intelligens f√∂r avancerad analys"""

    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=2000, stop_words="english")
        self.cluster_model = KMeans(n_clusters=5, random_state=42)
        self.sentiment_scores = {}
        self.content_patterns = {}
        self.language_detector = self._init_language_detector()

    def _init_language_detector(self):
        """Initierar spr√•kdetektor"""
        swedish_words = set(
            [
                "√§r",
                "och",
                "f√∂r",
                "med",
                "den",
                "det",
                "som",
                "p√•",
                "att",
                "till",
                "av",
                "inte",
                "ett",
                "en",
                "jag",
                "han",
                "hon",
                "du",
                "vi",
                "de",
                "s√•",
                "men",
                "n√§r",
                "var",
                "kan",
                "ska",
                "vill",
                "m√•ste",
                "b√∂r",
                "f√•r",
            ]
        )
        english_words = set(
            [
                "the",
                "and",
                "for",
                "with",
                "this",
                "that",
                "are",
                "was",
                "were",
                "have",
                "has",
                "had",
                "will",
                "would",
                "could",
                "should",
                "may",
                "might",
                "can",
                "must",
            ]
        )
        return {"swedish": swedish_words, "english": english_words}

    def analyze_content_magnificently(self, text: str, url: str, html_content: str) -> Dict[str, Any]:
        """Magnifik AI-analys av inneh√•ll"""
        return {
            "sentiment": self._analyze_sentiment_advanced(text),
            "topics": self._extract_topics_intelligent(text),
            "entities": self._extract_entities_advanced(text),
            "language": self._detect_language_advanced(text),
            "quality_score": self._calculate_quality_magnificent(text, url),
            "spam_probability": self._detect_spam_advanced(text, url),
            "content_type": self._classify_content_type(text, html_content),
            "readability": self._calculate_readability(text),
            "engagement_potential": self._calculate_engagement_potential(text),
            "seo_score": self._calculate_seo_score(html_content),
            "security_indicators": self._detect_security_indicators(url, html_content),
            "accessibility_score": self._calculate_accessibility_score(html_content),
            "performance_metrics": self._analyze_performance_metrics(html_content),
        }

    def _analyze_sentiment_advanced(self, text: str) -> Dict[str, float]:
        """Avancerad sentiment-analys"""
        positive_words = {
            "svenska": [
                "bra",
                "fantastisk",
                "utm√§rkt",
                "perfekt",
                "underbar",
                "fantastiskt",
                "grymt",
                "super",
                "awesome",
            ],
            "english": [
                "great",
                "amazing",
                "excellent",
                "perfect",
                "wonderful",
                "fantastic",
                "awesome",
                "super",
                "brilliant",
            ],
        }
        negative_words = {
            "svenska": [
                "d√•lig",
                "hemsk",
                "fruktansv√§rd",
                "skr√§p",
                "usel",
                "kass",
                "tr√•kig",
                "trist",
            ],
            "english": [
                "bad",
                "terrible",
                "awful",
                "horrible",
                "terrible",
                "poor",
                "boring",
                "dull",
            ],
        }

        words = text.lower().split()
        language = self._detect_language_advanced(text)

        pos_words = positive_words.get(language, positive_words["english"])
        neg_words = negative_words.get(language, negative_words["english"])

        positive_count = sum(1 for word in words if word in pos_words)
        negative_count = sum(1 for word in words if word in neg_words)
        total_words = len(words)

        if total_words == 0:
            return {"score": 0.0, "confidence": 0.0, "polarity": "neutral"}

        sentiment_score = (positive_count - negative_count) / total_words
        confidence = min(1.0, (positive_count + negative_count) / total_words * 10)

        if sentiment_score > 0.1:
            polarity = "positive"
        elif sentiment_score < -0.1:
            polarity = "negative"
        else:
            polarity = "neutral"

        return {
            "score": sentiment_score,
            "confidence": confidence,
            "polarity": polarity,
            "positive_words": positive_count,
            "negative_words": negative_count,
        }

    def _extract_topics_intelligent(self, text: str) -> List[Dict[str, Any]]:
        """Intelligent √§mnesextraktion"""
        topics = [
            {
                "name": "Teknologi",
                "keywords": [
                    "tech",
                    "computer",
                    "software",
                    "digital",
                    "ai",
                    "machine learning",
                ],
            },
            {
                "name": "Ekonomi",
                "keywords": [
                    "business",
                    "finance",
                    "money",
                    "economy",
                    "investment",
                    "market",
                ],
            },
            {
                "name": "Sport",
                "keywords": [
                    "sport",
                    "football",
                    "basketball",
                    "tennis",
                    "olympics",
                    "championship",
                ],
            },
            {
                "name": "Underh√•llning",
                "keywords": [
                    "entertainment",
                    "movie",
                    "music",
                    "celebrity",
                    "show",
                    "performance",
                ],
            },
            {
                "name": "Nyheter",
                "keywords": [
                    "news",
                    "politics",
                    "government",
                    "world",
                    "breaking",
                    "latest",
                ],
            },
            {
                "name": "H√§lsa",
                "keywords": [
                    "health",
                    "medical",
                    "doctor",
                    "hospital",
                    "treatment",
                    "wellness",
                ],
            },
            {
                "name": "Utbildning",
                "keywords": [
                    "education",
                    "school",
                    "university",
                    "learning",
                    "study",
                    "course",
                ],
            },
        ]

        text_lower = text.lower()
        detected_topics = []

        for topic in topics:
            keyword_matches = sum(1 for keyword in topic["keywords"] if keyword in text_lower)
            if keyword_matches > 0:
                confidence = min(1.0, keyword_matches / len(topic["keywords"]))
                detected_topics.append(
                    {
                        "name": topic["name"],
                        "confidence": confidence,
                        "keyword_matches": keyword_matches,
                    }
                )

        return sorted(detected_topics, key=lambda x: x["confidence"], reverse=True)[:3]

    def _extract_entities_advanced(self, text: str) -> Dict[str, List[str]]:
        """Avancerad entitetsextraktion"""
        # Enkel regex-baserad entitetsextraktion
        entities = {
            "personer": re.findall(r"\b[A-Z][a-z]+ [A-Z][a-z]+\b", text),
            "organisationer": re.findall(r"\b[A-Z][a-zA-Z\s&]+(?:Inc|Corp|Ltd|AB|AS|GmbH)\b", text),
            "platser": re.findall(r"\b[A-Z][a-z]+(?:stad|land|berg|dal|vik)\b", text),
            "datum": re.findall(r"\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b", text),
            "email": re.findall(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b", text),
            "urls": re.findall(r"https?://[^\s]+", text),
        }

        # Ta bort duplicerade entiteter
        for key in entities:
            entities[key] = list(set(entities[key]))[:10]  # Max 10 per kategori

        return entities

    def _detect_language_advanced(self, text: str) -> str:
        """Avancerad spr√•kdetektering"""
        words = text.lower().split()
        if len(words) < 5:
            return "unknown"

        swedish_count = sum(1 for word in words if word in self.language_detector["swedish"])
        english_count = sum(1 for word in words if word in self.language_detector["english"])

        if swedish_count > english_count and swedish_count > 2:
            return "swedish"
        elif english_count > swedish_count and english_count > 2:
            return "english"
        else:
            return "unknown"

    def _calculate_quality_magnificent(self, text: str, url: str) -> float:
        """Magnifik kvalitetsber√§kning"""
        if len(text) < 10:
            return 0.1

        quality_factors = {
            "length": min(1.0, len(text) / 5000),  # L√§ngre text = h√∂gre kvalitet
            "readability": self._calculate_readability(text),
            "structure": self._calculate_structure_score(text),
            "uniqueness": self._calculate_uniqueness_score(text),
            "domain_authority": self._calculate_domain_authority(url),
        }

        # Viktad genomsnitt
        weights = [0.3, 0.25, 0.2, 0.15, 0.1]
        quality_score = sum(factor * weight for factor, weight in zip(quality_factors.values(), weights))

        return min(1.0, quality_score)

    def _detect_spam_advanced(self, text: str, url: str) -> float:
        """Avancerad spam-detektering"""
        spam_indicators = [
            r"\b(?:klicka h√§r|gratis|vinn|lotteri|miljoner|dollars?|euros?)\b",
            r"\b(?:viagra|cialis|penis|sex|porn)\b",
            r"\b(?:casino|poker|blackjack|roulette)\b",
            r"\b(?:weight loss|diet|miracle|cure)\b",
            r"\$\$\$|\$\d+|\d+\$",
            r"\b(?:urgent|limited time|act now|don\'t miss)\b",
        ]

        text_lower = text.lower()
        spam_score = 0

        for pattern in spam_indicators:
            matches = len(re.findall(pattern, text_lower, re.IGNORECASE))
            spam_score += matches * 0.2

        # URL-baserad spam-detektering
        url_lower = url.lower()
        url_spam_indicators = ["casino", "porn", "viagra", "weight-loss", "make-money"]
        for indicator in url_spam_indicators:
            if indicator in url_lower:
                spam_score += 0.3

        return min(1.0, spam_score)

    def _classify_content_type(self, text: str, html_content: str) -> str:
        """Klassificerar inneh√•llstyp"""
        if "<script" in html_content.lower():
            return "application"
        elif "<form" in html_content.lower():
            return "form"
        elif len(re.findall(r"\b\d{1,2}:\d{2}\b", text)) > 2:
            return "schedule"
        elif len(re.findall(r"\$\d+", text)) > 3:
            return "ecommerce"
        elif len(re.findall(r"\b(?:news|breaking|latest)\b", text.lower())) > 2:
            return "news"
        else:
            return "article"

    def _calculate_readability(self, text: str) -> float:
        """Ber√§knar l√§sbarhet (Flesch Reading Ease)"""
        sentences = len(re.split(r"[.!?]+", text))
        words = len(text.split())
        syllables = len(re.findall(r"[aeiouy]+", text.lower()))

        if sentences == 0 or words == 0:
            return 0.5

        # F√∂renklad l√§sbarhetsformel
        readability = 206.835 - (1.015 * (words / sentences)) - (84.6 * (syllables / words))
        return max(0.0, min(1.0, readability / 100))

    def _calculate_structure_score(self, text: str) -> float:
        """Ber√§knar strukturskore"""
        paragraphs = len(text.split("\n\n"))
        sentences = len(re.split(r"[.!?]+", text))

        if sentences == 0:
            return 0.5

        avg_sentence_length = len(text.split()) / sentences
        structure_score = 0

        # Bel√∂na balanserad struktur
        if 10 <= avg_sentence_length <= 25:
            structure_score += 0.3
        if 3 <= paragraphs <= 20:
            structure_score += 0.3
        if len(text) > 500:
            structure_score += 0.4

        return min(1.0, structure_score)

    def _calculate_uniqueness_score(self, text: str) -> float:
        """Ber√§knar unikhetsskore"""
        words = text.lower().split()
        unique_words = set(words)

        if len(words) == 0:
            return 0.5

        uniqueness = len(unique_words) / len(words)
        return uniqueness

    def _calculate_domain_authority(self, url: str) -> float:
        """Ber√§knar dom√§nauktoritet"""
        try:
            domain = urlparse(url).netloc
            # Enkel heuristik baserad p√• dom√§nnamn
            if any(trusted in domain for trusted in ["google", "wikipedia", "github", "stackoverflow"]):
                return 0.9
            elif any(media in domain for media in ["bbc", "cnn", "nytimes", "svt", "dn"]):
                return 0.8
            elif domain.endswith(".edu") or domain.endswith(".gov"):
                return 0.7
            else:
                return 0.5
        except:
            return 0.5

    def _calculate_engagement_potential(self, text: str) -> float:
        """Ber√§knar engagemangspotential"""
        engagement_factors = {
            "questions": len(re.findall(r"\?", text)) * 0.1,
            "exclamations": len(re.findall(r"!", text)) * 0.05,
            "quotes": len(re.findall(r'["\']', text)) * 0.02,
            "numbers": len(re.findall(r"\d+", text)) * 0.01,
            "links": len(re.findall(r"http", text)) * 0.1,
        }

        engagement_score = sum(engagement_factors.values())
        return min(1.0, engagement_score)

    def _calculate_seo_score(self, html_content: str) -> float:
        """Ber√§knar SEO-skore"""
        seo_factors = {
            "title": 1 if "<title>" in html_content else 0,
            "meta_description": 1 if 'name="description"' in html_content else 0,
            "h1": len(re.findall(r"<h1[^>]*>", html_content)) * 0.2,
            "h2": len(re.findall(r"<h2[^>]*>", html_content)) * 0.1,
            "images_alt": len(re.findall(r'alt="[^"]*"', html_content)) * 0.05,
            "internal_links": len(re.findall(r'href="[^"]*"', html_content)) * 0.02,
        }

        seo_score = sum(seo_factors.values())
        return min(1.0, seo_score)

    def _detect_security_indicators(self, url: str, html_content: str) -> Dict[str, Any]:
        """Detekterar s√§kerhetsindikatorer"""
        security_indicators = {
            "https": 1 if url.startswith("https") else 0,
            "ssl_certificate": 1 if url.startswith("https") else 0,
            "secure_headers": 0,  # Skulle kr√§va headers-analys
            "suspicious_scripts": len(re.findall(r'<script[^>]*src="[^"]*"', html_content)),
            "external_links": len(re.findall(r'href="https?://[^"]*"', html_content)),
            "form_security": len(re.findall(r'<form[^>]*action="https?://[^"]*"', html_content)),
        }

        return security_indicators

    def _calculate_accessibility_score(self, html_content: str) -> float:
        """Ber√§knar tillg√§nglighetsskore"""
        accessibility_factors = {
            "alt_text": len(re.findall(r'alt="[^"]*"', html_content)) * 0.1,
            "aria_labels": len(re.findall(r'aria-label="[^"]*"', html_content)) * 0.2,
            "semantic_html": len(
                re.findall(
                    r"<(nav|main|article|section|aside|header|footer)[^>]*>",
                    html_content,
                )
            )
            * 0.1,
            "form_labels": len(re.findall(r"<label[^>]*>", html_content)) * 0.1,
        }

        accessibility_score = sum(accessibility_factors.values())
        return min(1.0, accessibility_score)

    def _analyze_performance_metrics(self, html_content: str) -> Dict[str, Any]:
        """Analyserar prestandam√•tt"""
        return {
            "image_count": len(re.findall(r"<img[^>]*>", html_content)),
            "script_count": len(re.findall(r"<script[^>]*>", html_content)),
            "css_count": len(re.findall(r'<link[^>]*rel="stylesheet"[^>]*>', html_content)),
            "inline_styles": len(re.findall(r'style="[^"]*"', html_content)),
            "external_resources": len(re.findall(r'src="https?://[^"]*"', html_content)),
        }


class MagnificentDatabase:
    """Magnifik databas f√∂r avancerad datahantering"""

    def __init__(self, db_path: str = "magnificent_scraper.db"):
        self.db_path = db_path
        self.init_database()

    def init_database(self):
        """Initierar den magnifika databasen"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # Huvudtabell f√∂r scraping-resultat
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS magnificent_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT NOT NULL,
                data_hash TEXT UNIQUE,
                data_compressed BLOB,
                metadata TEXT,
                ai_insights TEXT,
                security_analysis TEXT,
                performance_metrics TEXT,
                quality_score REAL,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """
        )

        # Tabell f√∂r AI-analyser
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS ai_analyses (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                result_id INTEGER,
                sentiment_score REAL,
                sentiment_polarity TEXT,
                topics TEXT,
                entities TEXT,
                language TEXT,
                spam_probability REAL,
                content_type TEXT,
                readability_score REAL,
                engagement_potential REAL,
                seo_score REAL,
                accessibility_score REAL,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (result_id) REFERENCES magnificent_results (id)
            )
        """
        )

        # Tabell f√∂r prestandam√•tt
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS performance_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                result_id INTEGER,
                response_time REAL,
                data_size INTEGER,
                image_count INTEGER,
                script_count INTEGER,
                css_count INTEGER,
                external_resources INTEGER,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (result_id) REFERENCES magnificent_results (id)
            )
        """
        )

        conn.commit()
        conn.close()

    def save_magnificent_result(self, result: ScrapingResult):
        """Sparar magnifikt resultat"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # Komprimera data
        data_compressed = gzip.compress(pickle.dumps(result.data))

        # Spara huvudresultat
        cursor.execute(
            """
            INSERT OR REPLACE INTO magnificent_results 
            (url, data_hash, data_compressed, metadata, ai_insights, security_analysis, performance_metrics, quality_score)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                result.url,
                result.hash,
                data_compressed,
                json.dumps(result.metadata),
                json.dumps(result.ai_insights),
                json.dumps(result.security_analysis),
                json.dumps(result.performance),
                result.quality_score,
            ),
        )

        result_id = cursor.lastrowid

        # Spara AI-analys
        if result.ai_insights:
            cursor.execute(
                """
                INSERT INTO ai_analyses 
                (result_id, sentiment_score, sentiment_polarity, topics, entities, language, 
                 spam_probability, content_type, readability_score, engagement_potential, seo_score, accessibility_score)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    result_id,
                    result.ai_insights.get("sentiment", {}).get("score", 0),
                    result.ai_insights.get("sentiment", {}).get("polarity", "neutral"),
                    json.dumps(result.ai_insights.get("topics", [])),
                    json.dumps(result.ai_insights.get("entities", {})),
                    result.ai_insights.get("language", "unknown"),
                    result.ai_insights.get("spam_probability", 0),
                    result.ai_insights.get("content_type", "unknown"),
                    result.ai_insights.get("readability", 0),
                    result.ai_insights.get("engagement_potential", 0),
                    result.ai_insights.get("seo_score", 0),
                    result.ai_insights.get("accessibility_score", 0),
                ),
            )

        # Spara prestandam√•tt
        cursor.execute(
            """
            INSERT INTO performance_metrics 
            (result_id, response_time, data_size, image_count, script_count, css_count, external_resources)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """,
            (
                result_id,
                result.performance.get("response_time", 0),
                result.performance.get("data_size", 0),
                result.ai_insights.get("performance_metrics", {}).get("image_count", 0),
                result.ai_insights.get("performance_metrics", {}).get("script_count", 0),
                result.ai_insights.get("performance_metrics", {}).get("css_count", 0),
                result.ai_insights.get("performance_metrics", {}).get("external_resources", 0),
            ),
        )

        conn.commit()
        conn.close()


class UltimateMagnificentScraper:
    """Det ultimata magnifika scraping-verktyget"""

    def __init__(self, mode: ScrapingMode = ScrapingMode.MAGNIFICENT):
        self.mode = mode
        self.ai = MagnificentAI()
        self.database = MagnificentDatabase()
        self.session = requests.Session()
        self.is_running = False
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_data_size": 0,
            "ai_analyses": 0,
            "quality_scores": [],
            "average_response_time": 0,
        }
        self.url_queue = queue.Queue()
        self.workers = []
        self._setup_session()

    def _setup_session(self):
        """Konfigurerar session f√∂r magnifik scraping"""
        # √Öterst√§ll till grundl√§ge
        self.session.headers.update(
            {
                "User-Agent": "UltimateMagnificentScraper/3.0 (Advanced Web Intelligence)",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "sv-SE,sv;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
            }
        )

        # L√§gg till mode-specifika headers
        if self.mode == ScrapingMode.STEALTH:
            self.session.headers.update(
                {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                    "DNT": "1",
                    "Sec-Fetch-Dest": "document",
                    "Sec-Fetch-Mode": "navigate",
                    "Sec-Fetch-Site": "none",
                    "Sec-Fetch-User": "?1",
                    "Cache-Control": "max-age=0",
                }
            )
        elif self.mode == ScrapingMode.AGGRESSIVE:
            self.session.headers.update(
                {
                    "User-Agent": "UltimateMagnificentScraper/3.0 (Aggressive Mode)",
                    "Cache-Control": "no-cache",
                    "Pragma": "no-cache",
                    "X-Requested-With": "XMLHttpRequest",
                    "X-Forwarded-For": "192.168.1.1",
                }
            )
        elif self.mode == ScrapingMode.INTELLIGENT:
            self.session.headers.update(
                {
                    "User-Agent": "UltimateMagnificentScraper/3.0 (Intelligent Mode)",
                    "X-Intelligent-Scraping": "true",
                    "X-AI-Enabled": "true",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                }
            )
        elif self.mode == ScrapingMode.AI_POWERED:
            self.session.headers.update(
                {
                    "User-Agent": "UltimateMagnificentScraper/3.0 (AI-Powered Mode)",
                    "X-AI-Enabled": "true",
                    "X-Intelligent-Scraping": "ai_powered",
                    "X-Machine-Learning": "enabled",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,application/json,*/*;q=0.8",
                }
            )
        elif self.mode == ScrapingMode.MAGNIFICENT:
            self.session.headers.update(
                {
                    "User-Agent": "UltimateMagnificentScraper/3.0 (Magnificent Mode)",
                    "X-Magnificent-Scraping": "true",
                    "X-Ultimate-Mode": "enabled",
                    "X-AI-Intelligence": "magnificent",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
                }
            )

    def start(self):
        """Startar den magnifika scrapern"""
        if not self.is_running:
            self.is_running = True

            # Starta worker-threads
            for i in range(3):
                worker = threading.Thread(target=self._worker_loop, args=(i,))
                worker.daemon = True
                worker.start()
                self.workers.append(worker)

            print("üåü Ultimate Magnificent Scraper startad!")
            print(f"üéØ Mode: {self.mode.value}")
            print(f"ü§ñ AI Intelligence: Magnifik")
            print(f"üíæ Database: Avancerad")
            print(f"üë• Workers: {len(self.workers)}")

    def stop(self):
        """Stoppar scrapern"""
        self.is_running = False
        print("üõë Ultimate Magnificent Scraper stoppad!")

    def add_url(self, url: str):
        """L√§gger till URL f√∂r magnifik scraping"""
        self.url_queue.put(url)
        print(f"‚ûï URL tillagd f√∂r magnifik scraping: {url}")

    def _worker_loop(self, worker_id: int):
        """Worker-loop f√∂r magnifik scraping"""
        while self.is_running:
            try:
                url = self.url_queue.get(timeout=1)
                self._scrape_url_magnificently(url)
                self.url_queue.task_done()
            except queue.Empty:
                continue
            except Exception as e:
                print(f"‚ùå Worker {worker_id} fel: {e}")

    def _scrape_url_magnificently(self, url: str):
        """Magnifik URL-scraping"""
        self.stats["total_requests"] += 1
        start_time = time.time()

        try:
            # L√§gg till https:// om det saknas
            if not url.startswith(("http://", "https://")):
                url = "https://" + url

            print(f"üåü Magnifik scraping: {url}")

            # G√∂r HTTP-request med avancerade headers
            headers = self._get_magnificent_headers()
            response = self.session.get(url, headers=headers, timeout=15)
            response.raise_for_status()

            response_time = time.time() - start_time

            # Parse HTML
            soup = BeautifulSoup(response.content, "html.parser")

            # Extrahera all data magnifikt
            data = self._extract_all_data_magnificently(soup, url)

            # Magnifik AI-analys
            ai_insights = self.ai.analyze_content_magnificently(data.get("text", ""), url, response.text)

            # Ber√§kna kvalitetsskore
            quality_score = ai_insights.get("quality_score", 0)

            # Skapa magnifikt resultat
            result = ScrapingResult(
                url=url,
                success=True,
                data=data,
                metadata={
                    "status_code": response.status_code,
                    "content_type": response.headers.get("content-type", ""),
                    "content_length": len(response.content),
                    "encoding": response.encoding,
                    "server": response.headers.get("server", ""),
                    "last_modified": response.headers.get("last-modified", ""),
                },
                performance={
                    "response_time": response_time,
                    "data_size": len(response.content),
                    "compression_ratio": len(response.content) / len(response.content),
                },
                ai_insights=ai_insights,
                security_analysis=ai_insights.get("security_indicators", {}),
                timestamp=datetime.now(),
                hash=hashlib.md5(response.content).hexdigest(),
                quality_score=quality_score,
            )

            # Spara i magnifik databas
            self.database.save_magnificent_result(result)

            # Uppdatera statistik
            self.stats["successful_requests"] += 1
            self.stats["total_data_size"] += len(response.content)
            self.stats["ai_analyses"] += 1
            self.stats["quality_scores"].append(quality_score)
            self.stats["average_response_time"] = (
                self.stats["average_response_time"] * (self.stats["successful_requests"] - 1) + response_time
            ) / self.stats["successful_requests"]

            print(f"‚úÖ Magnifik framg√•ng: {url}")
            print(f"   üéØ Kvalitet: {quality_score:.2f}")
            print(f"   üß† Sentiment: {ai_insights.get('sentiment', {}).get('polarity', 'neutral')}")
            print(f"   üìä √Ñmnen: {[t['name'] for t in ai_insights.get('topics', [])]}")

            # Visa Blocket-analys om det finns
            if "blocket.se" in url and "blocket_analysis" in data:
                blocket_data = data["blocket_analysis"]
                print(f"   üõí Blocket Analys:")
                print(f"      üìÇ Kategori: {blocket_data.get('category', 'Ok√§nd')}")
                print(f"      üì¶ Produkter: {blocket_data.get('product_count', 0)}")
                if blocket_data.get("price_range", {}).get("average", 0) > 0:
                    print(f"      üí∞ Genomsnittspris: {blocket_data['price_range']['average']} kr")
                if blocket_data.get("trending_keywords"):
                    print(f"      üî• Trendande: {', '.join(blocket_data['trending_keywords'][:3])}")

            # Visa social media-analys om det finns
            if (
                any(
                    platform in url
                    for platform in [
                        "google.com",
                        "facebook.com",
                        "twitter.com",
                        "instagram.com",
                        "linkedin.com",
                        "youtube.com",
                    ]
                )
                and "social_media_analysis" in data
            ):
                social_data = data["social_media_analysis"]
                print(f"   üì± Social Media Analys:")
                print(f"      üè¢ Plattform: {social_data.get('platform', 'Ok√§nd')}")
                if social_data.get("trending_hashtags"):
                    print(f"      #Ô∏è‚É£ Hashtags: {', '.join(social_data['trending_hashtags'][:3])}")
                if social_data.get("content_types"):
                    print(f"      üìπ Inneh√•ll: {', '.join(social_data['content_types'])}")
                if social_data.get("user_activity"):
                    activity = social_data["user_activity"]
                    print(f"      üë• Aktivitet: {activity.get('likes', 0)} likes, {activity.get('shares', 0)} shares")

                # Google-specifik info
                if "google.com" in url and "google_specific" in social_data:
                    google_data = social_data["google_specific"]
                    if google_data.get("news_headlines"):
                        print(f"      üì∞ Nyheter: {google_data['news_headlines'][0][:50]}...")

        except Exception as e:
            self.stats["failed_requests"] += 1
            print(f"‚ùå Magnifik fel vid scraping {url}: {e}")

    def _extract_all_data_magnificently(self, soup, url: str) -> Dict[str, Any]:
        """Extraherar all data magnifikt"""
        data = {
            "text": self._extract_text_magnificently(soup),
            "links": self._extract_links_magnificently(soup, url),
            "images": self._extract_images_magnificently(soup, url),
            "forms": self._extract_forms_magnificently(soup),
            "tables": self._extract_tables_magnificently(soup),
            "metadata": self._extract_metadata_magnificently(soup),
            "json_data": self._extract_json_data_magnificently(soup),
            "structured_data": self._extract_structured_data_magnificently(soup),
        }

        # Specialanalys f√∂r Blocket
        if "blocket.se" in url:
            data["blocket_analysis"] = self._analyze_blocket_specifically(soup, url)

        # Specialanalys f√∂r sociala medier
        if any(
            platform in url
            for platform in [
                "google.com",
                "facebook.com",
                "twitter.com",
                "instagram.com",
                "linkedin.com",
                "youtube.com",
            ]
        ):
            data["social_media_analysis"] = self._analyze_social_media_specifically(soup, url)

        return data

    def _extract_text_magnificently(self, soup) -> str:
        """Extraherar text magnifikt"""
        # Ta bort script och style
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()

        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = " ".join(chunk for chunk in chunks if chunk)

        return text

    def _extract_links_magnificently(self, soup, base_url: str) -> List[Dict[str, str]]:
        """Extraherar l√§nkar magnifikt"""
        links = []
        for link in soup.find_all("a", href=True):
            href = link["href"]
            if href.startswith("/"):
                href = urljoin(base_url, href)
            elif not href.startswith("http"):
                href = urljoin(base_url, href)

            links.append(
                {
                    "url": href,
                    "text": link.get_text().strip(),
                    "title": link.get("title", ""),
                    "rel": link.get("rel", []),
                    "target": link.get("target", ""),
                }
            )

        return links

    def _extract_images_magnificently(self, soup, base_url: str) -> List[Dict[str, str]]:
        """Extraherar bilder magnifikt"""
        images = []
        for img in soup.find_all("img"):
            src = img.get("src", "")
            if src.startswith("/"):
                src = urljoin(base_url, src)
            elif not src.startswith("http"):
                src = urljoin(base_url, src)

            images.append(
                {
                    "url": src,
                    "alt": img.get("alt", ""),
                    "title": img.get("title", ""),
                    "width": img.get("width", ""),
                    "height": img.get("height", ""),
                    "loading": img.get("loading", ""),
                }
            )

        return images

    def _extract_forms_magnificently(self, soup) -> List[Dict[str, Any]]:
        """Extraherar formul√§r magnifikt"""
        forms = []
        for form in soup.find_all("form"):
            form_data = {
                "action": form.get("action", ""),
                "method": form.get("method", "get"),
                "enctype": form.get("enctype", ""),
                "fields": [],
            }

            for field in form.find_all(["input", "textarea", "select"], recursive=False):
                field_data = {
                    "type": field.get("type", field.name),
                    "name": field.get("name", ""),
                    "value": field.get("value", ""),
                    "placeholder": field.get("placeholder", ""),
                    "required": field.get("required") is not None,
                    "disabled": field.get("disabled") is not None,
                }
                form_data["fields"].append(field_data)

            forms.append(form_data)

        return forms

    def _extract_tables_magnificently(self, soup) -> List[Dict[str, Any]]:
        """Extraherar tabeller magnifikt"""
        tables = []
        for table in soup.find_all("table"):
            table_data = {
                "headers": [],
                "rows": [],
                "caption": (table.find("caption").get_text() if table.find("caption") else ""),
            }

            headers = table.find_all("th")
            if headers:
                table_data["headers"] = [h.get_text().strip() for h in headers]

            for row in table.find_all("tr"):
                cells = row.find_all(["td", "th"], recursive=False)
                if cells:
                    table_data["rows"].append([cell.get_text().strip() for cell in cells])

            tables.append(table_data)

        return tables

    def _extract_metadata_magnificently(self, soup) -> Dict[str, str]:
        """Extraherar metadata magnifikt"""
        metadata = {}

        # Meta-taggar
        for meta in soup.find_all("meta"):
            name = meta.get("name", meta.get("property", ""))
            content = meta.get("content", "")
            if name and content:
                metadata[name] = content

        # Titel
        title = soup.find("title")
        if title:
            metadata["title"] = title.get_text()

        # Open Graph
        for og in soup.find_all("meta", attrs={"property": re.compile(r"^og:")}):
            metadata[og.get("property")] = og.get("content", "")

        # Twitter Cards
        for twitter in soup.find_all("meta", attrs={"name": re.compile(r"^twitter:")}):
            metadata[twitter.get("name")] = twitter.get("content", "")

        return metadata

    def _extract_json_data_magnificently(self, soup) -> List[Dict[str, Any]]:
        """Extraherar JSON-data magnifikt"""
        json_data = []

        # Script-taggar med JSON
        for script in soup.find_all("script", attrs={"type": "application/ld+json"}):
            try:
                data = json.loads(script.string)
                json_data.append(data)
            except:
                continue

        # Script-taggar med JSON-variabler
        for script in soup.find_all("script"):
            if script.string:
                # Leta efter JSON i script
                json_matches = re.findall(r"\{[^{}]*\}", script.string)
                for match in json_matches:
                    try:
                        data = json.loads(match)
                        json_data.append(data)
                    except:
                        continue

        return json_data

    def _extract_structured_data_magnificently(self, soup) -> Dict[str, Any]:
        """Extraherar strukturerad data magnifikt"""
        structured_data = {"schema_org": [], "microdata": [], "rdfa": []}

        # Schema.org
        for item in soup.find_all(attrs={"itemtype": True}):
            structured_data["schema_org"].append(
                {
                    "type": item.get("itemtype"),
                    "properties": {
                        prop.get("itemprop"): prop.get("content", prop.get_text())
                        for prop in item.find_all(attrs={"itemprop": True}, recursive=False)
                    },
                }
            )

        return structured_data

    def _analyze_blocket_specifically(self, soup, url: str) -> Dict[str, Any]:
        """Specialanalys f√∂r Blocket-webbplatser"""
        analysis = {
            "category": self._detect_blocket_category(url),
            "product_count": 0,
            "price_range": {"min": 0, "max": 0, "average": 0},
            "popular_products": [],
            "location_analysis": {},
            "trending_keywords": [],
        }

        # R√§kna produkter/annonser
        product_elements = soup.find_all(["article", "div"], class_=re.compile(r"(product|item|listing|annons)"))
        analysis["product_count"] = len(product_elements)

        # Extrahera priser
        prices = []
        price_elements = soup.find_all(text=re.compile(r"\d+\s*kr|\d+\s*:-|\d+\s*SEK"))
        for price_text in price_elements:
            price_match = re.search(r"(\d+)", price_text)
            if price_match:
                prices.append(int(price_match.group(1)))

        if prices:
            analysis["price_range"] = {
                "min": min(prices),
                "max": max(prices),
                "average": sum(prices) // len(prices),
            }

        # Analysera popul√§ra produkter
        product_titles = []
        title_elements = soup.find_all(["h1", "h2", "h3", "h4"], class_=re.compile(r"(title|heading)"))
        for title in title_elements:
            if title.get_text().strip():
                product_titles.append(title.get_text().strip())

        analysis["popular_products"] = product_titles[:10]  # Top 10

        # Platsanalys
        location_elements = soup.find_all(
            text=re.compile(
                r"(Stockholm|G√∂teborg|Malm√∂|Uppsala|V√§ster√•s|√ñrebro|Link√∂ping|Helsingborg|J√∂nk√∂ping|Norrk√∂ping)"
            )
        )
        analysis["location_analysis"] = {"locations_found": list(set(location_elements))}

        # Trendande nyckelord
        all_text = soup.get_text().lower()
        trending_words = re.findall(r"\b[a-z√•√§√∂]{4,}\b", all_text)
        word_freq = {}
        for word in trending_words:
            if word not in [
                "s√§ljes",
                "k√∂pes",
                "annons",
                "blocket",
                "se",
                "com",
                "www",
                "http",
            ]:
                word_freq[word] = word_freq.get(word, 0) + 1

        # Top 10 trendande ord
        trending = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        analysis["trending_keywords"] = [word for word, count in trending]

        return analysis

    def _analyze_social_media_specifically(self, soup, url: str) -> Dict[str, Any]:
        """Specialanalys f√∂r sociala medier och Google"""
        analysis = {
            "platform": self._detect_social_platform(url),
            "trending_topics": [],
            "engagement_metrics": {},
            "content_types": [],
            "user_activity": {},
            "trending_hashtags": [],
            "popular_content": [],
        }

        # Analysera trending topics
        trending_elements = soup.find_all(text=re.compile(r"(trending|popular|viral|hot|top|latest)"))
        analysis["trending_topics"] = [elem.strip() for elem in trending_elements[:10]]

        # Analysera hashtags
        hashtag_pattern = r"#\w+"
        hashtags = re.findall(hashtag_pattern, soup.get_text())
        analysis["trending_hashtags"] = list(set(hashtags))[:10]

        # Analysera inneh√•llstyper
        content_types = []
        if soup.find_all("img"):
            content_types.append("images")
        if soup.find_all("video"):
            content_types.append("videos")
        if soup.find_all("audio"):
            content_types.append("audio")
        if soup.find_all("iframe"):
            content_types.append("embeds")
        analysis["content_types"] = content_types

        # Analysera anv√§ndaraktivitet
        activity_indicators = {
            "likes": len(re.findall(r"(like|gilla|thumbs)", soup.get_text().lower())),
            "shares": len(re.findall(r"(share|dela|retweet)", soup.get_text().lower())),
            "comments": len(re.findall(r"(comment|kommentar)", soup.get_text().lower())),
            "followers": len(re.findall(r"(follower|f√∂ljare)", soup.get_text().lower())),
        }
        analysis["user_activity"] = activity_indicators

        # Analysera popul√§rt inneh√•ll
        popular_content = []
        for link in soup.find_all("a", href=True):
            link_text = link.get_text().strip()
            if len(link_text) > 10 and len(link_text) < 100:
                popular_content.append(link_text)
        analysis["popular_content"] = popular_content[:10]

        # Google-specifik analys
        if "google.com" in url:
            analysis["google_specific"] = self._analyze_google_specifically(soup, url)

        return analysis

    def _analyze_google_specifically(self, soup, url: str) -> Dict[str, Any]:
        """Google-specifik analys"""
        google_analysis = {
            "search_trends": [],
            "suggested_searches": [],
            "news_headlines": [],
            "featured_content": [],
        }

        # Analysera s√∂ktrender
        search_elements = soup.find_all(["input", "div"], attrs={"placeholder": re.compile(r"search|s√∂k")})
        for elem in search_elements:
            placeholder = elem.get("placeholder", "")
            if placeholder:
                google_analysis["search_trends"].append(placeholder)

        # Analysera f√∂reslagna s√∂kningar
        suggestion_elements = soup.find_all("div", class_=re.compile(r"suggestion|autocomplete"))
        for elem in suggestion_elements:
            text = elem.get_text().strip()
            if text:
                google_analysis["suggested_searches"].append(text)

        # Analysera nyhetsrubriker
        headline_elements = soup.find_all(["h1", "h2", "h3"], class_=re.compile(r"headline|title|news"))
        for elem in headline_elements:
            text = elem.get_text().strip()
            if text and len(text) > 10:
                google_analysis["news_headlines"].append(text)

        return google_analysis

    def _detect_social_platform(self, url: str) -> str:
        """Detekterar social media-plattform"""
        if "google.com" in url:
            return "Google"
        elif "facebook.com" in url:
            return "Facebook"
        elif "twitter.com" in url:
            return "Twitter/X"
        elif "instagram.com" in url:
            return "Instagram"
        elif "linkedin.com" in url:
            return "LinkedIn"
        elif "youtube.com" in url:
            return "YouTube"
        else:
            return "Unknown"

    def _detect_blocket_category(self, url: str) -> str:
        """Detekterar Blocket-kategori fr√•n URL"""
        if "/bilar" in url:
            return "Bilar"
        elif "/elektronik" in url:
            return "Elektronik"
        elif "/hem-garden" in url:
            return "Hem & Tr√§dg√•rd"
        elif "/sport-fritid" in url:
            return "Sport & Fritid"
        elif "/jobb" in url:
            return "Jobb"
        elif "/bostad" in url:
            return "Bostad"
        else:
            return "Allm√§nt"

    def _get_magnificent_headers(self) -> Dict[str, str]:
        """Returnerar magnifika headers baserat p√• l√§ge"""
        base_headers = self.session.headers.copy()

        # L√§gg till mode-specifika headers
        if self.mode == ScrapingMode.STEALTH:
            base_headers.update(
                {
                    "Referer": "https://www.google.com/",
                    "Sec-Ch-Ua": '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                    "Sec-Ch-Ua-Mobile": "?0",
                    "Sec-Ch-Ua-Platform": '"Windows"',
                }
            )
        elif self.mode == ScrapingMode.AGGRESSIVE:
            base_headers.update(
                {
                    "Cache-Control": "no-cache",
                    "Pragma": "no-cache",
                    "X-Requested-With": "XMLHttpRequest",
                    "X-Forwarded-For": "192.168.1.1",
                }
            )
        elif self.mode == ScrapingMode.INTELLIGENT:
            base_headers.update(
                {
                    "X-Intelligent-Scraping": "true",
                    "X-AI-Enabled": "true",
                    "X-Content-Analysis": "enabled",
                }
            )
        elif self.mode == ScrapingMode.AI_POWERED:
            base_headers.update(
                {
                    "X-AI-Enabled": "true",
                    "X-Intelligent-Scraping": "ai_powered",
                    "X-Machine-Learning": "enabled",
                    "X-Neural-Network": "active",
                }
            )
        elif self.mode == ScrapingMode.MAGNIFICENT:
            base_headers.update(
                {
                    "X-Magnificent-Scraping": "true",
                    "X-Ultimate-Mode": "enabled",
                    "X-AI-Intelligence": "magnificent",
                    "X-Quantum-Processing": "active",
                }
            )

        return base_headers

    def get_stats(self):
        """Returnerar magnifik statistik"""
        stats = self.stats.copy()
        if stats["quality_scores"]:
            stats["average_quality"] = sum(stats["quality_scores"]) / len(stats["quality_scores"])
        else:
            stats["average_quality"] = 0
        return stats


# ============================================================================
# üåü ULTIMATE DASHBOARD - Magnifik UI
# ============================================================================

app = FastAPI(title="üåü Ultimate Magnificent Scraper", version="3.0")

# Global scraper
magnificent_scraper = UltimateMagnificentScraper()


@app.get("/")
async def magnificent_dashboard():
    """Magnifik dashboard-huvudsida"""
    html = """
    <!DOCTYPE html>
    <html lang="sv">
    <head>
        <title>üåü Ultimate Magnificent Scraper</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <style>
            body {
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #f093fb 100%);
                color: white;
                margin: 0;
                padding: 20px;
                min-height: 100vh;
            }
            
            .container {
                max-width: 1400px;
                margin: 0 auto;
            }
            
            .header {
                text-align: center;
                margin-bottom: 40px;
            }
            
            .header h1 {
                font-size: 4em;
                margin: 0;
                text-shadow: 3px 3px 6px rgba(0,0,0,0.3);
                background: linear-gradient(45deg, #fff, #f0f0f0, #ffd700);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                background-clip: text;
            }
            
            .header p {
                font-size: 1.3em;
                margin: 10px 0;
                color: #fbbf24;
            }
            
            .controls-panel {
                background: rgba(255,255,255,0.1);
                padding: 30px;
                border-radius: 20px;
                -webkit-backdrop-filter: blur(20px);
                backdrop-filter: blur(20px);
                border: 1px solid rgba(255,255,255,0.2);
                margin-bottom: 30px;
            }
            
            .mode-selector {
                display: flex;
                justify-content: center;
                gap: 15px;
                margin-bottom: 20px;
                flex-wrap: wrap;
            }
            
            .mode-btn {
                background: rgba(255,255,255,0.2);
                color: white;
                border: none;
                padding: 15px 25px;
                border-radius: 25px;
                cursor: pointer;
                font-size: 1em;
                transition: all 0.3s;
            }
            
            .mode-btn:hover {
                background: rgba(255,255,255,0.3);
                transform: translateY(-3px);
            }
            
            .mode-btn.active {
                background: linear-gradient(45deg, #4ade80, #22c55e);
            }
            
            .url-input {
                display: flex;
                gap: 15px;
                margin-bottom: 20px;
                justify-content: center;
            }
            
            .url-input input {
                flex: 1;
                max-width: 500px;
                padding: 15px 20px;
                border: none;
                border-radius: 25px;
                font-size: 1em;
                background: rgba(255,255,255,0.2);
                color: white;
            }
            
            .url-input input::placeholder {
                color: rgba(255,255,255,0.7);
            }
            
            .action-buttons {
                display: flex;
                justify-content: center;
                gap: 15px;
                flex-wrap: wrap;
            }
            
            .btn {
                background: linear-gradient(45deg, #667eea, #764ba2);
                color: white;
                border: none;
                padding: 15px 30px;
                border-radius: 25px;
                cursor: pointer;
                font-size: 1em;
                font-weight: bold;
                transition: all 0.3s;
            }
            
            .btn:hover {
                transform: translateY(-3px);
            }
            
            .btn.danger {
                background: linear-gradient(45deg, #ef4444, #dc2626);
            }
            
            .btn.success {
                background: linear-gradient(45deg, #10b981, #059669);
            }
            
            .btn.magnificent {
                background: linear-gradient(45deg, #fbbf24, #f59e0b);
            }
            
            .stats-grid {
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
                gap: 20px;
                margin-bottom: 30px;
            }
            
            .stat-card {
                background: rgba(255,255,255,0.1);
                padding: 25px;
                border-radius: 20px;
                -webkit-backdrop-filter: blur(20px);
                backdrop-filter: blur(20px);
                border: 1px solid rgba(255,255,255,0.2);
                transition: all 0.3s;
            }
            
            .stat-card:hover {
                transform: translateY(-5px);
            }
            
            .stat-card h3 {
                margin: 0 0 15px 0;
                font-size: 1.3em;
                color: #fbbf24;
            }
            
            .stat-value {
                font-size: 2.5em;
                font-weight: bold;
                margin: 10px 0;
            }
            
            .success { color: #4ade80; }
            .error { color: #f87171; }
            .info { color: #60a5fa; }
            .magnificent { color: #fbbf24; }
            
            .ai-insights {
                background: rgba(255,255,255,0.1);
                padding: 25px;
                border-radius: 20px;
                -webkit-backdrop-filter: blur(20px);
                backdrop-filter: blur(20px);
                border: 1px solid rgba(255,255,255,0.2);
                margin-bottom: 30px;
            }
            
            .ai-insights h3 {
                margin: 0 0 20px 0;
                font-size: 1.5em;
                color: #a78bfa;
            }
            
            .insight-grid {
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                gap: 15px;
            }
            
            .insight-item {
                background: rgba(255,255,255,0.05);
                padding: 15px;
                border-radius: 10px;
                border-left: 4px solid #a78bfa;
            }
            
            .insight-item h4 {
                margin: 0 0 10px 0;
                color: #c4b5fd;
            }
            
            .progress-bar {
                width: 100%;
                height: 8px;
                background: rgba(255,255,255,0.2);
                border-radius: 4px;
                overflow: hidden;
                margin-top: 10px;
            }
            
            .progress-fill {
                height: 100%;
                background: linear-gradient(45deg, #4ade80, #22c55e);
                transition: width 0.3s ease;
            }
            
            .demo-urls {
                background: rgba(255,255,255,0.1);
                padding: 20px;
                border-radius: 15px;
                -webkit-backdrop-filter: blur(10px);
                backdrop-filter: blur(10px);
                margin-top: 20px;
            }
            
            .demo-urls h3 {
                margin: 0 0 15px 0;
                color: #fbbf24;
            }
            
            .demo-urls button {
                background: rgba(255,255,255,0.2);
                color: white;
                border: none;
                padding: 8px 15px;
                border-radius: 5px;
                cursor: pointer;
                margin: 5px;
                transition: all 0.3s;
            }
            
            .demo-urls button:hover {
                background: rgba(255,255,255,0.3);
            }
            
            .platform-section {
                margin-bottom: 20px;
                padding: 15px;
                background: rgba(255,255,255,0.05);
                border-radius: 10px;
                border: 1px solid rgba(255,255,255,0.1);
            }
            
            .platform-section h4 {
                margin: 0 0 10px 0;
                color: #fbbf24;
                font-size: 1.1em;
                border-bottom: 1px solid rgba(255,255,255,0.2);
                padding-bottom: 5px;
            }
            
            .platform-section button {
                background: rgba(255,255,255,0.15);
                color: white;
                border: 1px solid rgba(255,255,255,0.2);
                padding: 8px 12px;
                border-radius: 6px;
                cursor: pointer;
                margin: 3px;
                transition: all 0.3s;
                font-size: 0.9em;
            }
            
            .platform-section button:hover {
                background: rgba(255,255,255,0.25);
                transform: translateY(-2px);
                box-shadow: 0 4px 8px rgba(0,0,0,0.2);
            }
            
            .export-buttons {
                display: flex;
                gap: 10px;
                justify-content: center;
                margin-top: 10px;
            }
            
            .export-buttons .btn {
                flex: 1;
                max-width: 150px;
            }
            
            .quick-add-buttons {
                display: flex;
                gap: 10px;
                justify-content: center;
                margin-top: 15px;
                flex-wrap: wrap;
            }
            
            .quick-add-buttons .btn {
                flex: 1;
                max-width: 200px;
                font-size: 0.9em;
            }
            
            .current-mode {
                margin-top: 20px;
                padding: 15px;
                background: rgba(255,255,255,0.1);
                border-radius: 15px;
                -webkit-backdrop-filter: blur(10px);
                backdrop-filter: blur(10px);
                border: 1px solid rgba(255,255,255,0.2);
            }
            
            .mode-display {
                font-weight: bold;
                color: #fbbf24;
                font-size: 1.2em;
            }
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h1>üåü Ultimate Magnificent Scraper</h1>
                <p>Det verkliga ultimata scraping-verktyget med AI-intelligens</p>
                <p>Magnifikt, utomordentligt, med alla de b√§sta funktionerna!</p>
                <div class="current-mode">
                    <span>Aktivt l√§ge: </span>
                    <span id="currentModeDisplay" class="mode-display">üåü Magnificent</span>
                </div>
                <button onclick="testClick()" style="background: #ef4444; color: white; padding: 10px; border: none; border-radius: 5px; margin-top: 10px; cursor: pointer;">üß™ Testa om JavaScript fungerar</button>
                <button onclick="testAddUrl()" style="background: #10b981; color: white; padding: 10px; border: none; border-radius: 5px; margin-top: 10px; margin-left: 10px; cursor: pointer;">üîó Testa l√§gg till URL</button>
            </div>
            
            <div class="controls-panel">
                <div class="mode-selector">
                    <button class="mode-btn active" onclick="setMode('magnificent')">üåü Magnificent</button>
                    <button class="mode-btn" onclick="setMode('intelligent')">üß† Intelligent</button>
                    <button class="mode-btn" onclick="setMode('aggressive')">‚ö° Aggressiv</button>
                    <button class="mode-btn" onclick="setMode('stealth')">üïµÔ∏è Stealth</button>
                    <button class="mode-btn" onclick="setMode('ai_powered')">ü§ñ AI-Powered</button>
                </div>
                
                <div class="url-input">
                    <input type="text" id="urlInput" placeholder="Ange URL f√∂r magnifik scraping..." />
                    <button class="btn magnificent" onclick="addUrl()">üåü L√§gg till</button>
                </div>
                
                <div class="action-buttons">
                    <button class="btn success" onclick="startScraping()">üöÄ Starta Magnifik Scraping</button>
                    <button class="btn danger" onclick="stopScraping()">üõë Stoppa</button>
                    <div class="export-buttons">
                        <button class="btn" onclick="exportData('json')">üìÑ JSON Export</button>
                        <button class="btn" onclick="exportData('csv')">üìä CSV Export</button>
                    </div>
                    
                    <div class="quick-add-buttons">
                        <button class="btn magnificent" onclick="addPopularPlatforms()">üöÄ L√§gg till alla popul√§ra</button>
                        <button class="btn" onclick="addSocialMedia()">üì± Sociala Medier</button>
                        <button class="btn" onclick="addNewsSites()">üì∞ Nyhetssidor</button>
                        <button class="btn" onclick="showAIInsights()">üß† Visa AI-insikter</button>
                        <button class="btn" onclick="testClick()" style="background: #ef4444;">üß™ Test JavaScript</button>
                    </div>
                </div>
                
                <div class="demo-urls">
                    <h3>üéØ Popul√§ra plattformar - Klicka f√∂r att l√§gga till:</h3>
                    
                    <div class="platform-section">
                        <h4>üîç Google & S√∂kmotorer</h4>
                        <button onclick="addDemoUrl('google.com')">Google</button>
                        <button onclick="addDemoUrl('google.com/trends')">Google Trends</button>
                        <button onclick="addDemoUrl('google.com/news')">Google News</button>
                    </div>
                    
                    <div class="platform-section">
                        <h4>üì± Sociala Medier</h4>
                        <button onclick="addDemoUrl('facebook.com')">Facebook</button>
                        <button onclick="addDemoUrl('twitter.com')">Twitter/X</button>
                        <button onclick="addDemoUrl('instagram.com')">Instagram</button>
                        <button onclick="addDemoUrl('linkedin.com')">LinkedIn</button>
                        <button onclick="addDemoUrl('youtube.com')">YouTube</button>
                    </div>
                    
                    <div class="platform-section">
                        <h4>üõí E-handel & Marknadsplatser</h4>
                        <button onclick="addDemoUrl('blocket.se')">Blocket</button>
                        <button onclick="addDemoUrl('blocket.se/bilar')">Blocket Bilar</button>
                        <button onclick="addDemoUrl('blocket.se/elektronik')">Blocket Elektronik</button>
                        <button onclick="addDemoUrl('blocket.se/hem-garden')">Blocket Hem & Tr√§dg√•rd</button>
                        <button onclick="addDemoUrl('amazon.se')">Amazon Sverige</button>
                    </div>
                    
                    <div class="platform-section">
                        <h4>üì∞ Nyheter & Media</h4>
                        <button onclick="addDemoUrl('svt.se')">SVT</button>
                        <button onclick="addDemoUrl('dn.se')">Dagens Nyheter</button>
                        <button onclick="addDemoUrl('aftonbladet.se')">Aftonbladet</button>
                        <button onclick="addDemoUrl('expressen.se')">Expressen</button>
                    </div>
                    
                    <div class="platform-section">
                        <h4>üß™ Test & Demo</h4>
                        <button onclick="addDemoUrl('httpbin.org/html')">httpbin.org/html</button>
                        <button onclick="addDemoUrl('httpbin.org/json')">httpbin.org/json</button>
                        <button onclick="addDemoUrl('example.com')">example.com</button>
                    </div>
                </div>
            </div>
            
            <div class="stats-grid">
                <div class="stat-card">
                    <h3>üìä Total Requests</h3>
                    <div class="stat-value info" id="totalRequests">0</div>
                </div>
                <div class="stat-card">
                    <h3>‚úÖ Framg√•ngsrika</h3>
                    <div class="stat-value success" id="successfulRequests">0</div>
                </div>
                <div class="stat-card">
                    <h3>‚ùå Misslyckade</h3>
                    <div class="stat-value error" id="failedRequests">0</div>
                </div>
                <div class="stat-card">
                    <h3>üìà Framg√•ngsgrad</h3>
                    <div class="stat-value success" id="successRate">0%</div>
                </div>
                <div class="stat-card">
                    <h3>ü§ñ AI Analyser</h3>
                    <div class="stat-value info" id="aiAnalyses">0</div>
                </div>
                <div class="stat-card">
                    <h3>üíæ Data Storlek</h3>
                    <div class="stat-value info" id="dataSize">0 MB</div>
                </div>
                <div class="stat-card">
                    <h3>‚≠ê Genomsnittlig Kvalitet</h3>
                    <div class="stat-value magnificent" id="averageQuality">0.0</div>
                </div>
                <div class="stat-card">
                    <h3>‚ö° Genomsnittlig Svarstid</h3>
                    <div class="stat-value info" id="averageResponseTime">0.0s</div>
                </div>
            </div>
            
            <div class="ai-insights">
                <h3>üß† Magnifik AI Intelligens & Analys</h3>
                <div class="insight-grid">
                    <div class="insight-item">
                        <h4>Sentiment Score</h4>
                        <div class="stat-value" id="sentimentScore">0.0</div>
                        <div class="progress-bar">
                            <div class="progress-fill" id="sentimentBar" style="width: 50%"></div>
                        </div>
                    </div>
                    <div class="insight-item">
                        <h4>Kvalitetspo√§ng</h4>
                        <div class="stat-value" id="qualityScore">0.0</div>
                        <div class="progress-bar">
                            <div class="progress-fill" id="qualityBar" style="width: 50%"></div>
                        </div>
                    </div>
                    <div class="insight-item">
                        <h4>Spam Sannolikhet</h4>
                        <div class="stat-value" id="spamProbability">0.0</div>
                        <div class="progress-bar">
                            <div class="progress-fill" id="spamBar" style="width: 50%"></div>
                        </div>
                    </div>
                    <div class="insight-item">
                        <h4>L√§sbarhet</h4>
                        <div class="stat-value" id="readability">0.0</div>
                        <div class="progress-bar">
                            <div class="progress-fill" id="readabilityBar" style="width: 50%"></div>
                        </div>
                    </div>
                    <div class="insight-item">
                        <h4>SEO Score</h4>
                        <div class="stat-value" id="seoScore">0.0</div>
                        <div class="progress-bar">
                            <div class="progress-fill" id="seoBar" style="width: 50%"></div>
                        </div>
                    </div>
                    <div class="insight-item">
                        <h4>Tillg√§nglighet</h4>
                        <div class="stat-value" id="accessibility">0.0</div>
                        <div class="progress-bar">
                            <div class="progress-fill" id="accessibilityBar" style="width: 50%"></div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <script>
            let currentMode = 'magnificent';
            
            // Test-funktion f√∂r att verifiera att JavaScript fungerar
            function testClick() {
                alert('‚úÖ JavaScript fungerar! Knapparna borde fungera nu.');
                console.log('JavaScript test - OK');
            }
            
            function testAddUrl() {
                const testUrl = 'example.com';
                fetch('/add_url', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({url: testUrl})
                }).then(response => response.json())
                .then(data => {
                    alert('‚úÖ Test URL tillagd: ' + data.message);
                }).catch(error => {
                    alert('‚ùå Fel vid test: ' + error);
                });
            }
            
            function setMode(mode) {
                currentMode = mode;
                document.querySelectorAll('.mode-btn').forEach(btn => btn.classList.remove('active'));
                event.target.classList.add('active');
                
                // Uppdatera visuell indikator
                const modeNames = {
                    'magnificent': 'üåü Magnificent',
                    'intelligent': 'üß† Intelligent', 
                    'aggressive': '‚ö° Aggressiv',
                    'stealth': 'üïµÔ∏è Stealth',
                    'ai_powered': 'ü§ñ AI-Powered'
                };
                document.getElementById('currentModeDisplay').textContent = modeNames[mode] || 'üåü Magnificent';
                
                // Skicka mode till servern
                fetch('/set_mode', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({mode: mode})
                }).then(response => response.json())
                .then(data => {
                    console.log('L√§ge √§ndrat:', data.message);
                });
            }
            
            function addUrl() {
                const url = document.getElementById('urlInput').value;
                if (url) {
                    fetch('/add_url', {
                        method: 'POST',
                        headers: {'Content-Type': 'application/json'},
                        body: JSON.stringify({url: url})
                    });
                    document.getElementById('urlInput').value = '';
                }
            }
            
            function addDemoUrl(url) {
                document.getElementById('urlInput').value = url;
                addUrl();
            }
            
            function addPopularPlatforms() {
                const popularUrls = [
                    'google.com',
                    'facebook.com',
                    'twitter.com',
                    'instagram.com',
                    'youtube.com',
                    'blocket.se',
                    'svt.se',
                    'dn.se'
                ];
                
                popularUrls.forEach(url => {
                    fetch('/add_url', {
                        method: 'POST',
                        headers: {'Content-Type': 'application/json'},
                        body: JSON.stringify({url: url})
                    });
                });
                
                alert('üöÄ Lagt till alla popul√§ra plattformar!');
            }
            
            function addSocialMedia() {
                const socialUrls = [
                    'facebook.com',
                    'twitter.com',
                    'instagram.com',
                    'linkedin.com',
                    'youtube.com',
                    'tiktok.com',
                    'snapchat.com'
                ];
                
                socialUrls.forEach(url => {
                    fetch('/add_url', {
                        method: 'POST',
                        headers: {'Content-Type': 'application/json'},
                        body: JSON.stringify({url: url})
                    });
                });
                
                alert('üì± Lagt till alla sociala medier!');
            }
            
            function addNewsSites() {
                const newsUrls = [
                    'svt.se',
                    'dn.se',
                    'aftonbladet.se',
                    'expressen.se',
                    'sydsvenskan.se',
                    'gp.se',
                    'nt.se'
                ];
                
                newsUrls.forEach(url => {
                    fetch('/add_url', {
                        method: 'POST',
                        headers: {'Content-Type': 'application/json'},
                        body: JSON.stringify({url: url})
                    });
                });
                
                alert('üì∞ Lagt till alla nyhetssidor!');
            }
            
            function showAIInsights() {
                fetch('/api/ai-insights')
                .then(response => response.json())
                .then(data => {
                    let message = 'üß† AI-insikter:\n\n';
                    
                    if (data.latest_insight) {
                        const insight = data.latest_insight;
                        message += `üìä Senaste analys (${insight.url}):\n`;
                        message += `üéØ Kvalitet: ${insight.quality_score.toFixed(2)}\n`;
                        message += `üß† Sentiment: ${insight.sentiment.polarity || 'neutral'}\n`;
                        message += `üìù Spr√•k: ${insight.language}\n`;
                        message += `üìä Spam: ${(insight.spam_probability * 100).toFixed(1)}%\n`;
                        message += `üìñ L√§sbarhet: ${insight.readability.toFixed(2)}\n`;
                        message += `üéØ SEO: ${insight.seo_score.toFixed(2)}\n`;
                        message += `‚ôø Tillg√§nglighet: ${insight.accessibility_score.toFixed(2)}\n`;
                        
                        if (insight.topics && insight.topics.length > 0) {
                            message += `üìã √Ñmnen: ${insight.topics.map(t => t.name).join(', ')}\n`;
                        }
                    }
                    
                    message += `\nüìà Totalt antal analyser: ${data.total_insights}`;
                    
                    alert(message);
                })
                .catch(error => {
                    console.error('Fel vid h√§mtning av AI-insikter:', error);
                    alert('‚ùå Kunde inte h√§mta AI-insikter');
                });
            }
            
            function startScraping() {
                fetch('/start', {method: 'POST'});
            }
            
            function stopScraping() {
                fetch('/stop', {method: 'POST'});
            }
            
            function exportData(format = 'json') {
                const endpoint = format === 'csv' ? '/export/csv' : '/export';
                const filename = format === 'csv' ? 'magnificent_scraper_data.csv' : 'magnificent_scraper_data.json';
                
                fetch(endpoint)
                .then(response => response.blob())
                .then(blob => {
                    const url = window.URL.createObjectURL(blob);
                    const a = document.createElement('a');
                    a.href = url;
                    a.download = filename;
                    a.click();
                });
            }
            
            function updateStats(data) {
                if (data.stats) {
                    document.getElementById('totalRequests').textContent = data.stats.total_requests;
                    document.getElementById('successfulRequests').textContent = data.stats.successful_requests;
                    document.getElementById('failedRequests').textContent = data.stats.failed_requests;
                    
                    const successRate = data.stats.total_requests > 0 ? 
                        (data.stats.successful_requests / data.stats.total_requests * 100).toFixed(1) : 0;
                    document.getElementById('successRate').textContent = successRate + '%';
                    
                    document.getElementById('aiAnalyses').textContent = data.stats.ai_analyses;
                    document.getElementById('dataSize').textContent = (data.stats.total_data_size / 1024 / 1024).toFixed(2) + ' MB';
                    document.getElementById('averageQuality').textContent = data.stats.average_quality.toFixed(2);
                    document.getElementById('averageResponseTime').textContent = data.stats.average_response_time.toFixed(2) + 's';
                }
                
                // Uppdatera AI-insikter om de finns
                if (data.ai_insights) {
                    const ai = data.ai_insights;
                    
                    // Uppdatera v√§rden
                    document.getElementById('sentimentScore').textContent = (ai.sentiment?.score || 0).toFixed(2);
                    document.getElementById('qualityScore').textContent = (ai.quality_score || 0).toFixed(2);
                    document.getElementById('spamProbability').textContent = (ai.spam_probability || 0).toFixed(2);
                    document.getElementById('readability').textContent = (ai.readability || 0).toFixed(2);
                    document.getElementById('seoScore').textContent = (ai.seo_score || 0).toFixed(2);
                    document.getElementById('accessibility').textContent = (ai.accessibility_score || 0).toFixed(2);
                    
                    // Uppdatera progress bars
                    const sentimentPercent = ((ai.sentiment?.score || 0) + 1) / 2 * 100;
                    const qualityPercent = (ai.quality_score || 0) * 100;
                    const spamPercent = (ai.spam_probability || 0) * 100;
                    const readabilityPercent = (ai.readability || 0) * 100;
                    const seoPercent = (ai.seo_score || 0) * 100;
                    const accessibilityPercent = (ai.accessibility_score || 0) * 100;
                    
                    document.getElementById('sentimentBar').style.width = sentimentPercent + '%';
                    document.getElementById('qualityBar').style.width = qualityPercent + '%';
                    document.getElementById('spamBar').style.width = spamPercent + '%';
                    document.getElementById('readabilityBar').style.width = readabilityPercent + '%';
                    document.getElementById('seoBar').style.width = seoPercent + '%';
                    document.getElementById('accessibilityBar').style.width = accessibilityPercent + '%';
                    
                    console.log('AI-insikter uppdaterade:', ai);
                }
            }
            
            // Auto-refresh stats
            setInterval(() => {
                fetch('/api/stats')
                .then(response => response.json())
                .then(data => updateStats(data));
            }, 2000);
        </script>
    </body>
    </html>
    """
    return HTMLResponse(content=html)


@app.get("/test")
async def test_page():
    """Enkel test-sida f√∂r att verifiera JavaScript"""
    test_html = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>JavaScript Test</title>
    </head>
    <body>
        <h1>üß™ JavaScript Test</h1>
        <button onclick="testFunction()">Testa JavaScript</button>
        <button onclick="addTestUrl()">L√§gg till test URL</button>
        
        <script>
            function testFunction() {
                alert('‚úÖ JavaScript fungerar!');
                console.log('Test OK');
            }
            
            function addTestUrl() {
                fetch('/add_url', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({url: 'test.com'})
                }).then(response => response.json())
                .then(data => {
                    alert('‚úÖ URL tillagd: ' + data.message);
                }).catch(error => {
                    alert('‚ùå Fel: ' + error);
                });
            }
        </script>
    </body>
    </html>
    """
    return HTMLResponse(content=test_html)


@app.post("/start")
async def start_scraping():
    """Startar magnifik scraping"""
    magnificent_scraper.start()
    return {"status": "success", "message": "üåü Magnifik scraping startad!"}


@app.post("/stop")
async def stop_scraping():
    """Stoppar scraping"""
    magnificent_scraper.stop()
    return {"status": "success", "message": "üõë Scraping stoppad!"}


@app.post("/add_url")
async def add_url(request: Request):
    """L√§gger till URL f√∂r magnifik scraping"""
    data = await request.json()
    url = data.get("url")
    if url:
        magnificent_scraper.add_url(url)
        return {
            "status": "success",
            "message": f"URL tillagd f√∂r magnifik scraping: {url}",
        }
    return {"status": "error", "message": "Ingen URL angiven"}


@app.get("/api/stats")
async def get_stats():
    """API f√∂r magnifik statistik med AI-insikter"""
    import json

    # H√§mta grundl√§ggande statistik
    stats = magnificent_scraper.get_stats()

    # H√§mta senaste AI-insikter fr√•n databasen
    conn = sqlite3.connect("magnificent_scraper.db")
    cursor = conn.cursor()

    # H√§mta senaste resultat med AI-insikter
    cursor.execute(
        """
        SELECT ai_insights, quality_score
        FROM magnificent_results
        WHERE ai_insights IS NOT NULL
        ORDER BY timestamp DESC
        LIMIT 1
    """
    )

    row = cursor.fetchone()
    if row:
        ai_insights_raw, quality_score = row
        try:
            ai_insights = json.loads(ai_insights_raw)
            # Ber√§kna genomsnittliga AI-v√§rden
            stats["ai_insights"] = {
                "sentiment": {
                    "score": ai_insights.get("sentiment", {}).get("score", 0),
                    "polarity": ai_insights.get("sentiment", {}).get("polarity", "neutral"),
                },
                "quality_score": quality_score or 0,
                "spam_probability": ai_insights.get("spam_probability", 0),
                "readability": ai_insights.get("readability", 0),
                "engagement_potential": ai_insights.get("engagement_potential", 0),
                "seo_score": ai_insights.get("seo_score", 0),
                "accessibility_score": ai_insights.get("accessibility_score", 0),
                "topics": ai_insights.get("topics", []),
                "language": ai_insights.get("language", "unknown"),
                "content_type": ai_insights.get("content_type", "unknown"),
            }
        except:
            stats["ai_insights"] = {
                "sentiment": {"score": 0, "polarity": "neutral"},
                "quality_score": 0,
                "spam_probability": 0,
                "readability": 0,
                "engagement_potential": 0,
                "seo_score": 0,
                "accessibility_score": 0,
                "topics": [],
                "language": "unknown",
                "content_type": "unknown",
            }
    else:
        stats["ai_insights"] = {
            "sentiment": {"score": 0, "polarity": "neutral"},
            "quality_score": 0,
            "spam_probability": 0,
            "readability": 0,
            "engagement_potential": 0,
            "seo_score": 0,
            "accessibility_score": 0,
            "topics": [],
            "language": "unknown",
            "content_type": "unknown",
        }

    conn.close()
    return stats


@app.post("/set_mode")
async def set_mode(request: Request):
    """√Ñndrar scraping-l√§ge"""
    data = await request.json()
    mode = data.get("mode", "magnificent")

    # Uppdatera scraper-l√§get
    if hasattr(magnificent_scraper, "mode"):
        magnificent_scraper.mode = ScrapingMode(mode)
        magnificent_scraper._setup_session()  # Uppdatera headers

    return {"status": "success", "message": f"L√§ge √§ndrat till: {mode}"}


@app.get("/api/ai-insights")
async def get_ai_insights():
    """API f√∂r detaljerade AI-insikter"""
    import json

    conn = sqlite3.connect("magnificent_scraper.db")
    cursor = conn.cursor()

    # H√§mta alla AI-insikter fr√•n senaste resultaten
    cursor.execute(
        """
        SELECT url, ai_insights, quality_score, timestamp
        FROM magnificent_results
        WHERE ai_insights IS NOT NULL
        ORDER BY timestamp DESC
        LIMIT 10
    """
    )

    insights = []
    for row in cursor.fetchall():
        url, ai_insights_raw, quality_score, timestamp = row
        try:
            ai_data = json.loads(ai_insights_raw)
            insights.append(
                {
                    "url": url,
                    "quality_score": quality_score,
                    "sentiment": ai_data.get("sentiment", {}),
                    "topics": ai_data.get("topics", []),
                    "language": ai_data.get("language", "unknown"),
                    "spam_probability": ai_data.get("spam_probability", 0),
                    "readability": ai_data.get("readability", 0),
                    "engagement_potential": ai_data.get("engagement_potential", 0),
                    "seo_score": ai_data.get("seo_score", 0),
                    "accessibility_score": ai_data.get("accessibility_score", 0),
                    "content_type": ai_data.get("content_type", "unknown"),
                    "timestamp": timestamp,
                }
            )
        except:
            continue

    conn.close()

    return {
        "total_insights": len(insights),
        "latest_insight": insights[0] if insights else None,
        "all_insights": insights,
    }


@app.get("/export")
async def export_data():
    """Exporterar magnifik data som JSON-nedladdningsfil"""

    # H√§mta data fr√•n databasen
    conn = sqlite3.connect("magnificent_scraper.db")
    cursor = conn.cursor()

    # H√§mta alla resultat
    cursor.execute(
        """
        SELECT url, metadata, ai_insights, security_analysis, performance_metrics, quality_score, timestamp
        FROM magnificent_results
        ORDER BY timestamp DESC
    """
    )

    results = []
    for row in cursor.fetchall():
        (
            url,
            metadata,
            ai_insights,
            security_analysis,
            performance_metrics,
            quality_score,
            timestamp,
        ) = row
        results.append(
            {
                "url": url,
                "metadata": json.loads(metadata) if metadata else {},
                "ai_insights": json.loads(ai_insights) if ai_insights else {},
                "security_analysis": (json.loads(security_analysis) if security_analysis else {}),
                "performance_metrics": (json.loads(performance_metrics) if performance_metrics else {}),
                "quality_score": quality_score,
                "timestamp": timestamp,
            }
        )

    conn.close()

    # Skapa export-data
    export_data = {
        "export_info": {
            "timestamp": datetime.now().isoformat(),
            "scraper_version": "Ultimate Magnificent Scraper 3.0",
            "total_results": len(results),
            "export_format": "JSON",
        },
        "scraper_stats": magnificent_scraper.get_stats(),
        "scraped_data": results,
        "features": [
            "Avancerad AI-intelligens",
            "Magnifik kvalitetsanalys",
            "S√§kerhetsanalys",
            "SEO-optimering",
            "Tillg√§nglighetsanalys",
            "Prestandam√•tt",
            "Strukturerad data-extraktion",
            "JSON och XML-parsning",
        ],
    }

    # Skapa filnamn med timestamp
    filename = f"magnificent_scraper_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

    # Returnera som nedladdningsfil
    from fastapi.responses import FileResponse

    # Skapa tempor√§r fil
    with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".json", prefix="magnificent_export_") as tmp_file:
        json.dump(export_data, tmp_file, indent=2, ensure_ascii=False)
        tmp_file_path = tmp_file.name

    return FileResponse(
        path=tmp_file_path,
        filename=filename,
        media_type="application/json",
        headers={"Content-Disposition": f'attachment; filename="{filename}"'},
    )


@app.get("/export/csv")
async def export_csv():
    """Exporterar magnifik data som CSV-nedladdningsfil"""
    import csv

    # H√§mta data fr√•n databasen
    conn = sqlite3.connect("magnificent_scraper.db")
    cursor = conn.cursor()

    # H√§mta alla resultat
    cursor.execute(
        """
        SELECT url, metadata, ai_insights, security_analysis, performance_metrics, quality_score, timestamp
        FROM magnificent_results
        ORDER BY timestamp DESC
    """
    )

    results = []
    for row in cursor.fetchall():
        (
            url,
            metadata,
            ai_insights,
            security_analysis,
            performance_metrics,
            quality_score,
            timestamp,
        ) = row
        results.append(
            {
                "url": url,
                "metadata": json.loads(metadata) if metadata else {},
                "ai_insights": json.loads(ai_insights) if ai_insights else {},
                "security_analysis": (json.loads(security_analysis) if security_analysis else {}),
                "performance_metrics": (json.loads(performance_metrics) if performance_metrics else {}),
                "quality_score": quality_score,
                "timestamp": timestamp,
            }
        )

    conn.close()

    # Skapa CSV-data
    csv_data = []
    for result in results:
        ai_insights = result["ai_insights"]
        csv_data.append(
            {
                "URL": result["url"],
                "Kvalitetspo√§ng": result["quality_score"],
                "Sentiment": ai_insights.get("sentiment", {}).get("polarity", "neutral"),
                "Spr√•k": ai_insights.get("language", "unknown"),
                "Spam-sannolikhet": ai_insights.get("spam_probability", 0),
                "Inneh√•llstyp": ai_insights.get("content_type", "unknown"),
                "L√§sbarhet": ai_insights.get("readability", 0),
                "Engagemangspotential": ai_insights.get("engagement_potential", 0),
                "SEO-po√§ng": ai_insights.get("seo_score", 0),
                "Tillg√§nglighetspo√§ng": ai_insights.get("accessibility_score", 0),
                "√Ñmnen": ", ".join([t["name"] for t in ai_insights.get("topics", [])]),
                "Timestamp": result["timestamp"],
            }
        )

    # Skapa filnamn
    filename = f"magnificent_scraper_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

    # Skapa tempor√§r CSV-fil
    with tempfile.NamedTemporaryFile(
        mode="w",
        delete=False,
        suffix=".csv",
        prefix="magnificent_export_",
        newline="",
        encoding="utf-8",
    ) as tmp_file:
        if csv_data:
            fieldnames = csv_data[0].keys()
            writer = csv.DictWriter(tmp_file, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(csv_data)
        tmp_file_path = tmp_file.name

    return FileResponse(
        path=tmp_file_path,
        filename=filename,
        media_type="text/csv",
        headers={"Content-Disposition": f'attachment; filename="{filename}"'},
    )


if __name__ == "__main__":
    print("üåü STARTAR ULTIMATE MAGNIFICENT SCRAPER!")
    print("=" * 70)
    print("üéØ Det verkliga ultimata scraping-verktyget")
    print("ü§ñ Magnifik AI-intelligens och avancerad analys")
    print("üíæ Avancerad databas med komprimering")
    print("üîí S√§kerhetsanalys och GDPR-compliance")
    print("üìä SEO-optimering och tillg√§nglighetsanalys")
    print("üé® Magnifik och responsiv UI")
    print("=" * 70)
    print("üåê √ñppna: http://127.0.0.1:8000")
    print("üöÄ Redo f√∂r magnifik scraping!")

    uvicorn.run(app, host="127.0.0.1", port=8000, log_level="info")
