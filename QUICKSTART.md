# üöÄ Snabbstart - Web Scraping Toolkit

## Installation

### 1. Klona och installera dependencies
```bash
# Navigera till projektmappen
cd scraping

# Installera Python-dependencies
pip install -r requirements.txt

# Installera Playwright browsers (valfritt, f√∂r JavaScript-rendering)
playwright install
```

### 2. Verifiera installation
```bash
# Testa att allt fungerar
python -c "from src.scraper import WebScraper; print('‚úÖ Installation lyckades!')"
```

## Snabbstart-exempel

### Exempel 1: Skrapa en enskild sida
```python
import asyncio
from src.scraper import WebScraper

async def main():
    async with WebScraper() as scraper:
        result = await scraper.scrape_url("https://httpbin.org/html")
        
        if result.success:
            print(f"‚úÖ Titel: {result.data.get('title', 'N/A')}")
            print(f"‚è±Ô∏è  Response-tid: {result.response_time:.2f}s")
            
            # Exportera data
            scraper.export_data(result, "json", "data/f√∂rsta_scraping.json")
        else:
            print(f"‚ùå Fel: {result.error}")

asyncio.run(main())
```

### Exempel 2: Batch-scraping av flera sidor
```python
import asyncio
from src.scraper import WebScraper

async def main():
    urls = [
        "https://httpbin.org/html",
        "https://httpbin.org/json",
        "https://httpbin.org/xml"
    ]
    
    async with WebScraper() as scraper:
        results = await scraper.scrape_multiple(urls)
        
        successful = [r for r in results if r.success]
        print(f"‚úÖ Framg√•ngsrika: {len(successful)}/{len(urls)}")
        
        # Exportera alla resultat
        scraper.export_data(results, "json", "data/batch_resultat.json")

asyncio.run(main())
```

### Exempel 3: Anv√§nd CLI-verktyget
```bash
# Skrapa enskild URL
python cli.py scrape https://httpbin.org/html --format json

# Skrapa flera URLs fr√•n fil
echo "https://httpbin.org/html" > urls.txt
echo "https://httpbin.org/json" >> urls.txt
python cli.py batch urls.txt --format csv

# Visa konfiguration
python cli.py config --show
```

## Konfiguration

### Skapa anpassad konfiguration
```bash
# Skapa exempelkonfiguration
python cli.py config --create

# Redigera config/config/my_config.yaml
# Anv√§nd din konfiguration
python cli.py scrape https://example.com --config config/my_config.yaml
```

### Viktiga konfigurationsinst√§llningar
```yaml
scraper:
  timeout: 30                    # Timeout i sekunder
  delay_between_requests: 1.0    # Paus mellan requests
  use_playwright: false          # Aktivera f√∂r JavaScript-sidor

rate_limiting:
  requests_per_minute: 60        # Max requests per minut
  max_concurrent: 10             # Samtidiga requests

logging:
  level: "INFO"                  # DEBUG, INFO, WARNING, ERROR
  format: "json"                 # json eller console
```

## Vanliga anv√§ndningsfall

### 1. Skrapa produktdata
```python
from src.scraper import WebScraper
from src.scraper.parsers import HTMLParser

async def scrape_products():
    async with WebScraper() as scraper:
        # Anv√§nd anpassade CSS-selektorer
        selectors = {
            "title": ".product-title",
            "price": ".product-price",
            "description": ".product-description"
        }
        
        result = await scraper.scrape_url("https://example-shop.com/product")
        
        if result.success:
            # Anv√§nd HTMLParser f√∂r anpassad extraktion
            parser = HTMLParser()
            custom_data = parser.parse(result.data.get('raw_html', ''), selectors)
            
            print(f"Produkt: {custom_data.get('title')}")
            print(f"Pris: {custom_data.get('price')}")
```

### 2. Validera skrapad data
```python
from src.scraper.validators import DataValidator

# Validera data
validator = DataValidator()
validated_data = validator.validate({
    "title": "Produktnamn",
    "email": "kontakt@f√∂retag.se",
    "price": "299.99"
})

# Kontrollera valideringsresultat
validation = validated_data.get('_validation', {})
print(f"Giltig data: {validation.get('overall_valid')}")
```

### 3. Exportera till olika format
```python
from src.scraper.exporters import DataExporter

exporter = DataExporter()

# Exportera till JSON
exporter.export(data, "json", "data/produkter.json")

# Exportera till CSV
exporter.export(data, "csv", "data/produkter.csv")

# Exportera till Excel
exporter.export(data, "excel", "data/produkter.xlsx")
```

## Fels√∂kning

### Vanliga problem och l√∂sningar

**Problem: "ModuleNotFoundError: No module named 'src'"**
```bash
# L√∂sning: L√§gg till src i Python-s√∂kv√§gen
export PYTHONPATH="${PYTHONPATH}:$(pwd)/src"
```

**Problem: "Playwright not installed"**
```bash
# L√∂sning: Installera Playwright browsers
playwright install
```

**Problem: "Rate limiting too aggressive"**
```yaml
# L√∂sning: Justera rate limiting i config
rate_limiting:
  requests_per_minute: 120  # √ñka fr√•n 60
  delay_between_requests: 0.5  # Minska fr√•n 1.0
```

**Problem: "Timeout errors"**
```yaml
# L√∂sning: √ñka timeout
scraper:
  timeout: 60  # √ñka fr√•n 30
  connect_timeout: 20  # √ñka fr√•n 10
```

## N√§sta steg

1. **L√§s dokumentationen**: Se `README.md` f√∂r detaljerad information
2. **K√∂r testerna**: `pytest src/tests/` f√∂r att verifiera funktionalitet
3. **Utforska exempel**: Se `example_usage.py` f√∂r fler anv√§ndningsexempel
4. **Anpassa konfiguration**: Skapa din egen config-fil f√∂r dina behov
5. **Ut√∂ka funktionalitet**: L√§gg till anpassade parsers eller validators

## Support

- **Dokumentation**: `README.md` och `PROJECT_SUMMARY.md`
- **Exempel**: `example_usage.py`
- **Tester**: `src/tests/test_scraper.py`
- **CLI-hj√§lp**: `python cli.py --help`

---

üéâ **Grattis!** Du √§r nu redo att b√∂rja med etisk och effektiv web scraping!
