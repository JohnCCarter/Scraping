# Web Scraping Toolkit

Ett robust och skalbart verktyg fÃ¶r web scraping med fokus pÃ¥ etik, prestanda och underhÃ¥llbarhet.

## ğŸ¯ Funktioner

- **Asynkron scraping** fÃ¶r hÃ¶g prestanda
- **Automatisk retry-logik** med exponential backoff
- **Rate limiting** fÃ¶r att respektera servrar
- **Flexibel konfiguration** via YAML/JSON
- **Strukturerad loggning** fÃ¶r debugging
- **Data-validering** och export
- **Proxy-stÃ¶d** fÃ¶r IP-rotation
- **JavaScript-rendering** med Playwright/Selenium

## ğŸš€ Snabbstart

### Installation

```bash
# Klona repository
git clone <repository-url>
cd scraping

# Installera dependencies
pip install -r requirements.txt

# Installera Playwright browsers
playwright install
```

### GrundlÃ¤ggande anvÃ¤ndning

```python
from scraper import WebScraper

# Skapa scraper-instans
scraper = WebScraper(config_path="config.yaml")

# Skrapa en sida
data = await scraper.scrape_url("https://example.com")

# Exportera data
scraper.export_data(data, format="json")
```

## ğŸ“ Projektstruktur

```
scraping/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ core.py          # Huvudscraper-klass
â”‚   â”‚   â”œâ”€â”€ parsers.py       # HTML-parsers
â”‚   â”‚   â”œâ”€â”€ validators.py    # Data-validering
â”‚   â”‚   â””â”€â”€ exporters.py     # Data-export
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py        # Konfigurationshantering
â”‚   â”‚   â”œâ”€â”€ logging.py       # Loggning
â”‚   â”‚   â””â”€â”€ helpers.py       # HjÃ¤lpfunktioner
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ test_scraper.py
â”‚       â””â”€â”€ test_parsers.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ default.yaml         # Standardkonfiguration
â”‚   â””â”€â”€ examples/
â”œâ”€â”€ data/                    # Skrapad data
â”œâ”€â”€ logs/                    # Loggfiler
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âš™ï¸ Konfiguration

Skapa en `config.yaml`-fil:

```yaml
scraper:
  user_agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
  timeout: 30
  max_retries: 3
  delay_between_requests: 1.0
  
rate_limiting:
  requests_per_minute: 60
  burst_size: 10
  
logging:
  level: "INFO"
  format: "json"
  
export:
  default_format: "json"
  output_dir: "./data"
```

## ğŸ§ª Testning

```bash
# KÃ¶r alla tester
pytest

# KÃ¶r med coverage
pytest --cov=src

# KÃ¶r specifika tester
pytest tests/test_scraper.py
```

## ğŸ“Š Ã–vervakning

Verktyget inkluderar inbyggd Ã¶vervakning:
- Request/response-tider
- Success/failure rates
- Data-volym per session
- MinnesanvÃ¤ndning

## ğŸ”’ SÃ¤kerhet och Etik

- Respekterar robots.txt
- Implementerar rate limiting
- Roterar User-Agent headers
- StÃ¶d fÃ¶r proxy-anvÃ¤ndning
- Loggar alla aktiviteter fÃ¶r granskning

## ğŸ¤ Bidrag

1. Fork repository
2. Skapa feature branch
3. Commit Ã¤ndringar
4. Push till branch
5. Skapa Pull Request

## ğŸ“„ Licens

MIT License - se LICENSE-fil fÃ¶r detaljer.
