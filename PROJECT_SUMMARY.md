# Web Scraping Toolkit - ProjektÃ¶versikt

## ğŸ¯ ProjektmÃ¥l
Ett robust, skalbart och etiskt web scraping-verktyg med fokus pÃ¥ prestanda, underhÃ¥llbarhet och respekt fÃ¶r webbplatser.

## ğŸ“ Projektstruktur

```
scraping/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â”œâ”€â”€ __init__.py          # Paket-initiering och exports
â”‚   â”‚   â”œâ”€â”€ core.py              # Huvudscraper-klass (WebScraper)
â”‚   â”‚   â”œâ”€â”€ parsers.py           # HTML och JSON parsers
â”‚   â”‚   â”œâ”€â”€ validators.py        # Data-validering
â”‚   â”‚   â””â”€â”€ exporters.py         # Data-export (JSON, CSV, Excel, XML)
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py          # Utilitetspaket
â”‚   â”‚   â”œâ”€â”€ config.py            # Konfigurationshantering
â”‚   â”‚   â””â”€â”€ logging.py           # Strukturerad loggning
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ __init__.py          # Testpaket
â”‚       â””â”€â”€ test_scraper.py      # Omfattande enhetstester
â”œâ”€â”€ config/
â”‚   â””â”€â”€ default.yaml             # Standardkonfiguration
â”œâ”€â”€ data/                        # Skrapad data (output)
â”œâ”€â”€ logs/                        # Loggfiler
â”œâ”€â”€ requirements.txt             # Python-dependencies
â”œâ”€â”€ README.md                    # Projektdokumentation
â”œâ”€â”€ cli.py                       # Kommandoradsverktyg
â”œâ”€â”€ example_usage.py             # AnvÃ¤ndningsexempel
â””â”€â”€ PROJECT_SUMMARY.md           # Denna fil
```

## ğŸ”§ KÃ¤rnkomponenter

### 1. WebScraper (core.py)
**Huvudklass fÃ¶r all scraping-funktionalitet**

**Funktioner:**
- **Asynkron scraping** med `aiohttp` fÃ¶r hÃ¶g prestanda
- **JavaScript-rendering** med Playwright fÃ¶r dynamiskt innehÃ¥ll
- **Rate limiting** fÃ¶r att respektera servrar
- **Automatisk retry-logik** med exponential backoff
- **Robots.txt-respekt** (grundlÃ¤ggande implementation)
- **Session-hantering** med proper cleanup
- **Metrics-samling** fÃ¶r Ã¶vervakning

**Viktiga metoder:**
```python
async def scrape_url(url: str, parser_type: str = "auto") -> ScrapingResult
async def scrape_multiple(urls: List[str]) -> List[ScrapingResult]
def export_data(data, format: str, output_path: str)
```

### 2. Parsers (parsers.py)
**Data-extraktion frÃ¥n olika innehÃ¥llstyper**

**HTMLParser:**
- CSS-selektor stÃ¶d
- Automatisk extraktion av vanliga element (titel, lÃ¤nkar, bilder)
- FormulÃ¤r-extraktion
- Text-rening och normalisering

**JSONParser:**
- JSON-innehÃ¥ll parsing
- JSON-LD structured data extraktion
- Meta-taggar extraktion

**DataExtractor:**
- Kombinerad extraktor fÃ¶r bÃ¥de HTML och JSON
- Automatisk innehÃ¥llstyp-detektering

### 3. Validators (validators.py)
**Data-kvalitetskontroll och validering**

**DataValidator:**
- E-post, telefon, URL, datum, pris validering
- TextlÃ¤ngd-kontroller
- HTML-tag och specialtecken-detektering
- Detaljerad valideringsrapportering

**SchemaValidator:**
- Schema-baserad validering
- Obligatoriska fÃ¤lt-kontroll
- Datatyp-validering

### 4. Exporters (exporters.py)
**Flexibel data-export till olika format**

**StÃ¶dda format:**
- **JSON**: Strukturerad data med pretty-printing
- **CSV**: Platt struktur med automatisk flattening
- **Excel**: Med pandas/openpyxl stÃ¶d
- **XML**: Hierarkisk struktur

**Funktioner:**
- Timestamp-baserade filnamn
- NÃ¤stlad data-flattening
- Export-sammanfattningar

### 5. Configuration (utils/config.py)
**Flexibel konfigurationshantering**

**Funktioner:**
- **Hierarkisk konfiguration**: Standard â†’ Fil â†’ MiljÃ¶variabler
- **StÃ¶d fÃ¶r YAML och JSON**
- **MiljÃ¶variabel-override**
- **Punktnotation** fÃ¶r enkel Ã¥tkomst
- **Validering** av konfigurationsvÃ¤rden

### 6. Logging (utils/logging.py)
**Strukturerad loggning fÃ¶r debugging och Ã¶vervakning**

**Funktioner:**
- **JSON-format** fÃ¶r maskinlÃ¤sbarhet
- **Console-format** fÃ¶r utveckling
- **Fil-rotation** fÃ¶r diskhantering
- **ScrapingLogger** med inbyggda metrics
- **Session-tracking** fÃ¶r batch-operationer

## âš™ï¸ Konfiguration

### Standardkonfiguration (config/default.yaml)
```yaml
scraper:
  user_agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
  timeout: 30
  max_retries: 3
  delay_between_requests: 1.0
  use_playwright: false

rate_limiting:
  requests_per_minute: 60
  max_concurrent: 10

logging:
  level: "INFO"
  format: "json"
  file: "logs/scraper.log"

export:
  default_format: "json"
  output_dir: "data"
```

## ğŸš€ AnvÃ¤ndning

### GrundlÃ¤ggande anvÃ¤ndning
```python
from scraper import WebScraper

async with WebScraper("config/default.yaml") as scraper:
    result = await scraper.scrape_url("https://example.com")
    scraper.export_data(result, "json", "data/output.json")
```

### Batch-scraping
```python
urls = ["https://example1.com", "https://example2.com"]
results = await scraper.scrape_multiple(urls)
```

### CLI-anvÃ¤ndning
```bash
# Skrapa enskild URL
python cli.py scrape https://example.com --format csv

# Batch-scraping
python cli.py batch urls.txt --config my_config.yaml

# Visa konfiguration
python cli.py config --show
```

## ğŸ§ª Testning

### Omfattande test-suite (src/tests/test_scraper.py)
- **Enhetstester** fÃ¶r alla komponenter
- **Mock-baserade tester** fÃ¶r HTTP-requests
- **Asynkrona tester** med pytest-asyncio
- **TemporÃ¤ra filer** fÃ¶r export-tester

**Testade komponenter:**
- ScrapingResult dataclass
- HTMLParser och JSONParser
- DataValidator och SchemaValidator
- DataExporter med alla format
- WebScraper med mocked HTTP-requests

## ğŸ“Š Ã–vervakning och Metrics

### Inbyggda metrics
- **Request/response-tider**
- **Success/failure rates**
- **Data-volym per session**
- **Rate limiting-statistik**

### Loggning
- **Strukturerad JSON-loggning**
- **Session-tracking**
- **Fel-kategorisering**
- **Performance-metrics**

## ğŸ”’ SÃ¤kerhet och Etik

### Etiska principer
- **Rate limiting** fÃ¶r att respektera servrar
- **Robots.txt-respekt** (grundlÃ¤ggande)
- **User-Agent-rotation**
- **Proxy-stÃ¶d** fÃ¶r IP-rotation
- **Transparent loggning** fÃ¶r granskning

### SÃ¤kerhetsfunktioner
- **Input-validering** (URL-sanitization)
- **Timeout-hantering**
- **Fel-isolering**
- **SÃ¤ker konfigurationshantering**

## ğŸ“¦ Dependencies

### Core dependencies
```txt
requests>=2.31.0          # HTTP-requests
beautifulsoup4>=4.12.0    # HTML-parsing
aiohttp>=3.9.0           # Asynkron HTTP
playwright>=1.40.0       # JavaScript-rendering
pandas>=2.1.0            # Data-manipulation
```

### Utvecklingsverktyg
```txt
pytest>=7.4.0            # Testning
black>=23.0.0            # Kod-formatering
flake8>=6.1.0            # Linting
mypy>=1.7.0              # Type checking
```

## ğŸ¯ FÃ¶rdelar med denna lÃ¶sning

### 1. **ModulÃ¤r design**
- Separation of concerns
- Enkel att utÃ¶ka och underhÃ¥lla
- Testbar arkitektur

### 2. **Asynkron prestanda**
- HÃ¶g genomstrÃ¶mning fÃ¶r batch-operationer
- Effektiv resursanvÃ¤ndning
- Skalbar arkitektur

### 3. **Robust felhantering**
- Retry-logik med exponential backoff
- Graceful degradation
- Detaljerad felrapportering

### 4. **Flexibel konfiguration**
- MiljÃ¶baserad konfiguration
- Runtime-anpassning
- Validering av instÃ¤llningar

### 5. **Omfattande testning**
- Enhetstester fÃ¶r alla komponenter
- Mock-baserade integrationstester
- Asynkron test-stÃ¶d

### 6. **Produktionsredo**
- Strukturerad loggning
- Metrics-samling
- CLI-verktyg
- Dokumentation

## ğŸš€ NÃ¤sta steg

### MÃ¶jliga fÃ¶rbÃ¤ttringar
1. **Avancerad robots.txt-parsing**
2. **Proxy-rotation med health checks**
3. **Distribuerad scraping** med Redis-queue
4. **Webhook-stÃ¶d** fÃ¶r real-time notifieringar
5. **Dashboard** fÃ¶r visualisering av metrics
6. **Plugin-system** fÃ¶r anpassade parsers
7. **Caching-lager** fÃ¶r upprepade requests
8. **Machine learning** fÃ¶r automatisk selektor-generering

### Skalbarhet
- **Horisontell skalning** med flera instanser
- **Vertikal skalning** med resursoptimering
- **Queue-baserad arkitektur** fÃ¶r stora datasets
- **Database-integration** fÃ¶r persistent storage

## ğŸ“ Slutsats

Detta web scraping-verktyg erbjuder en komplett, produktionsredo lÃ¶sning fÃ¶r etisk och effektiv web scraping. Med sin modulÃ¤ra arkitektur, omfattande testning och flexibla konfiguration Ã¤r det lÃ¤mpligt fÃ¶r bÃ¥de smÃ¥skaliga projekt och stora produktionsmiljÃ¶er.

Verktyget fÃ¶ljer bÃ¤sta praxis fÃ¶r web scraping och inkluderar alla nÃ¶dvÃ¤ndiga komponenter fÃ¶r att bygga robusta, skalbara scraping-lÃ¶sningar.
